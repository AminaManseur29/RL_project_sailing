{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e66641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the BaseAgent class\n",
    "from src.agents.q_learning_agent import QLearningAgent\n",
    "from src.agents.my_agents.enhanced_qlearning_agent import MonAgent1\n",
    "from src.agents.my_agents.sarsa_agent import SARSAAgent\n",
    "from src.agents.my_agents.expectedsarsa_agent import ExpectedSARSAAgent\n",
    "from src.agents.my_agents.dqn_agent import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ac2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_sailing import SailingEnv\n",
    "from initial_windfields import get_initial_windfield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c368d",
   "metadata": {},
   "source": [
    "### Testing the Agent's Validity\n",
    "\n",
    "Let's make the agent do a few steps to check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b4694e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running my agent 1 for 5 steps:\n",
      "Step 1: Action=0, Position=[16  1], Reward=0.0\n",
      "Step 2: Action=0, Position=[16  1], Reward=0.0\n",
      "Step 3: Action=0, Position=[16  2], Reward=0.0\n",
      "Step 4: Action=0, Position=[16  2], Reward=0.0\n",
      "Step 5: Action=4, Position=[16  1], Reward=0.0\n",
      "Running my SARSA agent for 5 steps:\n",
      "Step 1: Action=0, Position=[16  1], Reward=0.0\n",
      "Step 2: Action=0, Position=[16  1], Reward=0.0\n",
      "Step 3: Action=0, Position=[16  1], Reward=0.0\n",
      "Step 4: Action=0, Position=[16  1], Reward=0.0\n",
      "Step 5: Action=4, Position=[16  0], Reward=0.0\n",
      "Running my Expected SARSA agent for 5 steps:\n",
      "Step 1: Action=0, Position=[16  0], Reward=0.0\n",
      "Step 2: Action=0, Position=[16  0], Reward=0.0\n",
      "Step 3: Action=0, Position=[16  0], Reward=0.0\n",
      "Step 4: Action=0, Position=[16  0], Reward=0.0\n",
      "Step 5: Action=4, Position=[16  0], Reward=0.0\n",
      "Running my DQN agent for 5 steps:\n",
      "Step 1: Action=7, Position=[15  1], Reward=0.0\n",
      "Step 2: Action=6, Position=[14  1], Reward=0.0\n",
      "Step 3: Action=6, Position=[13  1], Reward=0.0\n",
      "Step 4: Action=7, Position=[11  2], Reward=0.0\n",
      "Step 5: Action=7, Position=[10  3], Reward=0.0\n"
     ]
    }
   ],
   "source": [
    "# Instead of validating the agent here, we'll just demonstrate it on a simple task\n",
    "from src.env_sailing import SailingEnv\n",
    "\n",
    "# Create a simple environment\n",
    "env = SailingEnv()\n",
    "observation, info = env.reset(seed=42)\n",
    "state_dim = len(observation)\n",
    "\n",
    "# Initialize our agents\n",
    "mon_agent1 = MonAgent1()\n",
    "mon_agent1.seed(42)\n",
    "\n",
    "sarsa_agent = SARSAAgent()\n",
    "sarsa_agent.seed(42)\n",
    "\n",
    "expectedsarsa_agent = ExpectedSARSAAgent()\n",
    "expectedsarsa_agent.seed(42)\n",
    "\n",
    "dqn_agent = DQNAgent(state_dim=state_dim)\n",
    "dqn_agent.seed(42)\n",
    "\n",
    "# Run the agent for a few steps\n",
    "print(\"Running my agent 1 for 5 steps:\")\n",
    "for i in range(5):\n",
    "    action = mon_agent1.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: Action={action}, Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "# Run the agent for a few steps\n",
    "print(\"Running my SARSA agent for 5 steps:\")\n",
    "for i in range(5):\n",
    "    action = sarsa_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: Action={action}, Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "# Run the agent for a few steps\n",
    "print(\"Running my Expected SARSA agent for 5 steps:\")\n",
    "for i in range(5):\n",
    "    action = expectedsarsa_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: Action={action}, Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "# Run the agent for a few steps\n",
    "print(\"Running my DQN agent for 5 steps:\")\n",
    "for i in range(5):\n",
    "    action = dqn_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: Action={action}, Position={info['position']}, Reward={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d67fbb",
   "metadata": {},
   "source": [
    "### Analysis of the Agent\n",
    "\n",
    "The `MonAgent` is extremely simple but illustrates the key requirements for a valid agent:\n",
    "\n",
    "1. **Inheritance**: It inherits from `BaseAgent`\n",
    "2. **Required Methods**: It implements all required methods (`act`, `reset`, `seed`)\n",
    "3. **Action Selection**: It always returns action `0` (North)\n",
    "4. **Simplicity**: It maintains no internal state and requires no complex logic\n",
    "\n",
    "This agent provides a good baseline, but it has obvious limitations:\n",
    "\n",
    "- It ignores wind conditions completely\n",
    "- It will struggle when the wind is coming from the North\n",
    "- It doesn't adapt its strategy based on the environment\n",
    "\n",
    "Let's test the naive agent to see how well it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c89e4456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the naive agent on the simple_static initial windfield:\n",
      "Step 10: Position=[15  8], Reward=0.0\n",
      "Step 20: Position=[15 12], Reward=0.0\n",
      "Step 30: Position=[15 13], Reward=0.0\n",
      "Step 40: Position=[15 13], Reward=0.0\n",
      "Step 50: Position=[14 14], Reward=0.0\n",
      "Step 60: Position=[14 14], Reward=0.0\n",
      "Step 70: Position=[14 13], Reward=0.0\n",
      "Step 80: Position=[14 14], Reward=0.0\n",
      "Step 90: Position=[15 13], Reward=0.0\n",
      "Step 100: Position=[15 14], Reward=0.0\n",
      "Step 110: Position=[15 15], Reward=0.0\n",
      "Step 120: Position=[15 15], Reward=0.0\n",
      "Step 130: Position=[15 16], Reward=0.0\n",
      "Step 140: Position=[15 16], Reward=0.0\n",
      "Step 150: Position=[15 16], Reward=0.0\n",
      "Step 160: Position=[15 15], Reward=0.0\n",
      "Step 170: Position=[15 16], Reward=0.0\n",
      "Step 180: Position=[14 17], Reward=0.0\n",
      "Step 190: Position=[13 19], Reward=0.0\n",
      "Step 200: Position=[12 19], Reward=0.0\n",
      "Step 210: Position=[12 20], Reward=0.0\n",
      "Step 220: Position=[12 19], Reward=0.0\n",
      "Step 230: Position=[12 19], Reward=0.0\n",
      "Step 240: Position=[12 18], Reward=0.0\n",
      "Step 250: Position=[13 18], Reward=0.0\n",
      "Step 260: Position=[12 18], Reward=0.0\n",
      "Step 270: Position=[12 19], Reward=0.0\n",
      "Step 280: Position=[12 19], Reward=0.0\n",
      "Step 290: Position=[12 20], Reward=0.0\n",
      "Step 300: Position=[12 20], Reward=0.0\n",
      "Step 310: Position=[11 21], Reward=0.0\n",
      "Step 320: Position=[12 21], Reward=0.0\n",
      "Step 330: Position=[12 21], Reward=0.0\n",
      "Step 340: Position=[12 21], Reward=0.0\n",
      "Step 350: Position=[12 22], Reward=0.0\n",
      "Step 360: Position=[11 23], Reward=0.0\n",
      "Step 370: Position=[12 24], Reward=0.0\n",
      "Step 380: Position=[12 24], Reward=0.0\n",
      "Step 390: Position=[12 24], Reward=0.0\n",
      "Step 400: Position=[12 25], Reward=0.0\n",
      "Step 410: Position=[11 26], Reward=0.0\n",
      "Step 420: Position=[12 26], Reward=0.0\n",
      "Step 430: Position=[13 26], Reward=0.0\n",
      "Step 440: Position=[13 26], Reward=0.0\n",
      "Step 450: Position=[13 26], Reward=0.0\n",
      "Step 460: Position=[13 27], Reward=0.0\n",
      "Step 470: Position=[14 26], Reward=0.0\n",
      "Step 480: Position=[14 27], Reward=0.0\n",
      "Step 490: Position=[14 27], Reward=0.0\n",
      "Step 500: Position=[14 28], Reward=0.0\n",
      "Step 510: Position=[14 28], Reward=0.0\n",
      "Step 520: Position=[13 28], Reward=0.0\n",
      "Step 530: Position=[13 28], Reward=0.0\n",
      "Step 540: Position=[12 27], Reward=0.0\n",
      "Step 550: Position=[11 28], Reward=0.0\n",
      "Step 560: Position=[11 28], Reward=0.0\n",
      "Step 570: Position=[11 29], Reward=0.0\n",
      "Step 580: Position=[11 29], Reward=0.0\n",
      "Step 590: Position=[11 30], Reward=0.0\n",
      "Step 600: Position=[11 31], Reward=0.0\n",
      "Step 610: Position=[10 31], Reward=0.0\n",
      "Step 620: Position=[10 31], Reward=0.0\n",
      "Step 630: Position=[ 9 31], Reward=0.0\n",
      "Step 640: Position=[ 9 30], Reward=0.0\n",
      "Step 650: Position=[ 9 30], Reward=0.0\n",
      "Step 660: Position=[10 31], Reward=0.0\n",
      "Step 670: Position=[11 30], Reward=0.0\n",
      "Step 680: Position=[11 31], Reward=0.0\n",
      "Step 690: Position=[11 31], Reward=0.0\n",
      "Step 700: Position=[11 31], Reward=0.0\n",
      "Step 710: Position=[11 31], Reward=0.0\n",
      "Step 720: Position=[10 31], Reward=0.0\n",
      "Step 730: Position=[10 31], Reward=0.0\n",
      "Step 740: Position=[ 9 31], Reward=0.0\n",
      "Step 750: Position=[ 8 31], Reward=0.0\n",
      "Step 760: Position=[ 9 30], Reward=0.0\n",
      "Step 770: Position=[ 7 30], Reward=0.0\n",
      "Step 780: Position=[ 7 30], Reward=0.0\n",
      "Step 790: Position=[ 7 31], Reward=0.0\n",
      "Step 800: Position=[ 8 30], Reward=0.0\n",
      "Step 810: Position=[ 7 30], Reward=0.0\n",
      "Step 820: Position=[ 7 31], Reward=0.0\n",
      "Step 830: Position=[ 7 31], Reward=0.0\n",
      "Step 840: Position=[ 7 31], Reward=0.0\n",
      "Step 850: Position=[ 7 31], Reward=0.0\n",
      "Step 860: Position=[ 7 31], Reward=0.0\n",
      "Step 870: Position=[ 7 31], Reward=0.0\n",
      "Step 880: Position=[ 7 30], Reward=0.0\n",
      "Step 890: Position=[ 7 31], Reward=0.0\n",
      "Step 900: Position=[ 8 30], Reward=0.0\n",
      "Step 910: Position=[ 8 31], Reward=0.0\n",
      "Step 920: Position=[ 8 31], Reward=0.0\n",
      "Step 930: Position=[ 8 31], Reward=0.0\n",
      "Step 940: Position=[ 6 31], Reward=0.0\n",
      "Step 950: Position=[ 6 31], Reward=0.0\n",
      "Step 960: Position=[ 6 31], Reward=0.0\n",
      "Step 970: Position=[ 6 29], Reward=0.0\n",
      "Step 980: Position=[ 6 29], Reward=0.0\n",
      "Step 990: Position=[ 4 28], Reward=0.0\n",
      "Step 1000: Position=[ 3 30], Reward=0.0\n",
      "\n",
      "Episode finished after 1000 steps with reward: 0.0\n",
      "Final position: [ 3 30]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "# observation, info = env.reset(seed=42)\n",
    "# state_dim = len(observation)\n",
    "mon_agent = ExpectedSARSAAgent()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = mon_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6501b",
   "metadata": {},
   "source": [
    "## Improving on the Naive Agent\n",
    "\n",
    "The naive agent provides a good starting point, but there are many ways to improve it:\n",
    "\n",
    "1. **Wind-Aware Agent**: Consider wind direction when choosing actions\n",
    "2. **Goal-Directed Agent**: Calculate the direction to the goal and choose actions accordingly\n",
    "3. **Physics-Based Agent**: Use sailing physics equations to determine the most efficient action\n",
    "\n",
    "The key insight for sailing is that certain directions relative to the wind are more efficient than others:\n",
    "\n",
    "- The sailing efficiency is highest when moving perpendicular to the wind (beam reach)\n",
    "- It's difficult to sail directly into the wind (the \"no-go zone\" - less than 45° to the wind)\n",
    "- The boat maintains momentum (inertia) between steps\n",
    "\n",
    "Before diving into reinforcement learning, consider implementing a simple rule-based agent that incorporates these physics principles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af3d32",
   "metadata": {},
   "source": [
    "### Training the Q-Learning Agent\n",
    "\n",
    "Now let's train our Q-learning agent on a simple initial windfield. We'll start with a small number of episodes (10) to demonstrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2aa07c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 10 episodes (debug run)...\n",
      "Episode 1: Steps=1000, Reward=0.0, Position=[ 8 30], Goal reached=True\n",
      "Episode 2: Steps=1000, Reward=0.0, Position=[ 1 30], Goal reached=True\n",
      "Episode 3: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 4: Steps=1000, Reward=0.0, Position=[ 1 24], Goal reached=True\n",
      "Episode 5: Steps=1000, Reward=0.0, Position=[ 4 30], Goal reached=True\n",
      "Episode 6: Steps=1000, Reward=0.0, Position=[ 0 27], Goal reached=True\n",
      "Episode 7: Steps=1000, Reward=0.0, Position=[ 4 31], Goal reached=True\n",
      "Episode 8: Steps=1000, Reward=0.0, Position=[ 7 29], Goal reached=True\n",
      "Episode 9: Steps=1000, Reward=0.0, Position=[ 1 31], Goal reached=True\n",
      "Episode 10: Steps=716, Reward=100.0, Position=[15 30], Goal reached=True\n",
      "\n",
      "Debug training completed!\n",
      "Q-table size: 215 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent\n",
    "ql_agent = QLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.2)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10  # Small number for debugging\n",
    "max_steps = 1000\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training with 10 episodes (debug run)...\")\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")\n",
    "    \n",
    "    # Update exploration rate (optional: decrease exploration over time)\n",
    "    ql_agent.exploration_rate = max(0.05, ql_agent.exploration_rate * 0.95)\n",
    "\n",
    "print(\"\\nDebug training completed!\")\n",
    "print(f\"Q-table size: {len(ql_agent.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9c9e9",
   "metadata": {},
   "source": [
    "### Full Training Run\n",
    "\n",
    "Now let's train our agent for more episodes to get better performance. This will take longer but should result in a more effective agent.\n",
    "\n",
    "*Note: You might want to adjust the number of episodes based on your available time. More episodes generally lead to better performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b3dc9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full training with 100 episodes...\n",
      "Episode 10/100: Success rate (last 10): 100.0%\n",
      "Episode 20/100: Success rate (last 10): 100.0%\n",
      "Episode 30/100: Success rate (last 10): 100.0%\n",
      "Episode 40/100: Success rate (last 10): 100.0%\n",
      "Episode 50/100: Success rate (last 10): 100.0%\n",
      "Episode 60/100: Success rate (last 10): 100.0%\n",
      "Episode 70/100: Success rate (last 10): 100.0%\n",
      "Episode 80/100: Success rate (last 10): 100.0%\n",
      "Episode 90/100: Success rate (last 10): 100.0%\n",
      "Episode 100/100: Success rate (last 10): 100.0%\n",
      "\n",
      "Training completed in 5.3 seconds!\n",
      "Success rate: 100.0%\n",
      "Average reward: 100.00\n",
      "Average steps: 202.8\n",
      "Q-table size: 514 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent for full training\n",
    "ql_agent_full = QLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent_full.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 100  # More episodes for better learning\n",
    "max_steps = 1000\n",
    "\n",
    "# Progress tracking\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting full training with 100 episodes...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent_full.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent_full.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent_full.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent_full.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Record metrics\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step+1)\n",
    "    success_history.append(done)\n",
    "    \n",
    "    # Update exploration rate (decrease over time)\n",
    "    ql_agent_full.exploration_rate = max(0.05, ql_agent_full.exploration_rate * 0.98)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode+1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(ql_agent_full.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4ff19",
   "metadata": {},
   "source": [
    "### Entraînement Agent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8519f19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full training with 100 episodes...\n",
      "Episode 10/100: Success rate (last 10): 100.0%\n",
      "Episode 20/100: Success rate (last 10): 100.0%\n",
      "Episode 30/100: Success rate (last 10): 100.0%\n",
      "Episode 40/100: Success rate (last 10): 100.0%\n",
      "Episode 50/100: Success rate (last 10): 100.0%\n",
      "Episode 60/100: Success rate (last 10): 100.0%\n",
      "Episode 70/100: Success rate (last 10): 100.0%\n",
      "Episode 80/100: Success rate (last 10): 100.0%\n",
      "Episode 90/100: Success rate (last 10): 100.0%\n",
      "Episode 100/100: Success rate (last 10): 100.0%\n",
      "\n",
      "Training completed in 6.5 seconds!\n",
      "Success rate: 100.0%\n",
      "Average reward: 98.00\n",
      "Average steps: 204.2\n",
      "Q-table size: 1006 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent for full training\n",
    "enhanced_ql_agent_full = MonAgent1(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "enhanced_ql_agent_full.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 100  # More episodes for better learning\n",
    "max_steps = 1000\n",
    "\n",
    "# Progress tracking\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting full training with 100 episodes...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = enhanced_ql_agent_full.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = enhanced_ql_agent_full.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = enhanced_ql_agent_full.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        enhanced_ql_agent_full.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Record metrics\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step+1)\n",
    "    success_history.append(done)\n",
    "    \n",
    "    # Update exploration rate (decrease over time)\n",
    "    enhanced_ql_agent_full.exploration_rate = max(0.05, enhanced_ql_agent_full.exploration_rate * 0.98)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode+1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(enhanced_ql_agent_full.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94cb65",
   "metadata": {},
   "source": [
    "### Entraînement de SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b527dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SARSA training with 100 episodes...\n",
      "Episode 10/100: Success rate (last 10): 100.0%\n",
      "Episode 20/100: Success rate (last 10): 100.0%\n",
      "Episode 30/100: Success rate (last 10): 100.0%\n",
      "Episode 40/100: Success rate (last 10): 100.0%\n",
      "Episode 50/100: Success rate (last 10): 100.0%\n",
      "Episode 60/100: Success rate (last 10): 100.0%\n",
      "Episode 70/100: Success rate (last 10): 100.0%\n",
      "Episode 80/100: Success rate (last 10): 100.0%\n",
      "Episode 90/100: Success rate (last 10): 100.0%\n",
      "Episode 100/100: Success rate (last 10): 100.0%\n",
      "\n",
      "Training completed in 5.6 seconds!\n",
      "Success rate: 100.0%\n",
      "Average reward: 97.00\n",
      "Average steps: 180.2\n",
      "Q-table size: 862 states\n"
     ]
    }
   ],
   "source": [
    "# Créer l'agent SARSA\n",
    "sarsa_agent = SARSAAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Fixer la graine pour reproductibilité\n",
    "np.random.seed(42)\n",
    "sarsa_agent.seed(42)\n",
    "\n",
    "# Environnement\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Paramètres\n",
    "num_episodes = 100\n",
    "max_steps = 1000\n",
    "\n",
    "# Historique\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "print(\"Starting SARSA training with 100 episodes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset(seed=episode)\n",
    "    state = sarsa_agent.discretize_state(observation)\n",
    "    action = sarsa_agent.act(observation)\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = sarsa_agent.discretize_state(next_observation)\n",
    "        next_action = sarsa_agent.act(next_observation)\n",
    "\n",
    "        # Mise à jour SARSA\n",
    "        sarsa_agent.learn(state, action, reward, next_state, next_action)\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step + 1)\n",
    "    success_history.append(done)\n",
    "\n",
    "    # Diminuer l'exploration progressivement\n",
    "    sarsa_agent.exploration_rate = max(0.05, sarsa_agent.exploration_rate * 0.98)\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode + 1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(sarsa_agent.q_table)} states\")\n",
    "\n",
    "# Sauvegarde du modèle\n",
    "sarsa_agent.save(\"outputs/sarsa_agent.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa2c0c",
   "metadata": {},
   "source": [
    "### Entraînement Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6bd05b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Expected SARSA training with 100 episodes...\n",
      "Episode 10/100: Success rate (last 10): 100.0%\n",
      "Episode 20/100: Success rate (last 10): 100.0%\n",
      "Episode 30/100: Success rate (last 10): 100.0%\n",
      "Episode 40/100: Success rate (last 10): 100.0%\n",
      "Episode 50/100: Success rate (last 10): 100.0%\n",
      "Episode 60/100: Success rate (last 10): 100.0%\n",
      "Episode 70/100: Success rate (last 10): 100.0%\n",
      "Episode 80/100: Success rate (last 10): 100.0%\n",
      "Episode 90/100: Success rate (last 10): 100.0%\n",
      "Episode 100/100: Success rate (last 10): 100.0%\n",
      "\n",
      "Training completed in 6.3 seconds!\n",
      "Success rate: 100.0%\n",
      "Average reward: 98.00\n",
      "Average steps: 197.2\n",
      "Q-table size: 1011 states\n"
     ]
    }
   ],
   "source": [
    "# Créer l'agent Expected SARSA\n",
    "expected_agent = ExpectedSARSAAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Fixer la graine\n",
    "np.random.seed(42)\n",
    "expected_agent.seed(42)\n",
    "\n",
    "# Environnement\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "num_episodes = 100\n",
    "max_steps = 1000\n",
    "\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "print(\"Starting Expected SARSA training with 100 episodes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset(seed=episode)\n",
    "    state = expected_agent.discretize_state(observation)\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = expected_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = expected_agent.discretize_state(next_observation)\n",
    "\n",
    "        expected_agent.learn(state, action, reward, next_state)\n",
    "\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step + 1)\n",
    "    success_history.append(done)\n",
    "\n",
    "    # Diminution progressive d'epsilon\n",
    "    expected_agent.exploration_rate = max(0.05, expected_agent.exploration_rate * 0.98)\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode + 1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(expected_agent.q_table)} states\")\n",
    "\n",
    "# Sauvegarde\n",
    "expected_agent.save(\"outputs/expected_sarsa_agent.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23479d5e",
   "metadata": {},
   "source": [
    "### Entraînement Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89e82e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting DQN training with 100 episodes...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (64, 5) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m next_obs, reward, done, truncated, info = env.step(action)\n\u001b[32m     36\u001b[39m agent.remember(obs, action, reward, next_obs, done)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m obs = next_obs\n\u001b[32m     40\u001b[39m total_reward += reward\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/RL_project_sailing/src/agents/my_agents/dqn_agent.py:70\u001b[39m, in \u001b[36mDQNAgent.learn\u001b[39m\u001b[34m(self, batch_size)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.memory) < batch_size:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m batch = \u001b[38;5;28mzip\u001b[39m(*\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     71\u001b[39m states, actions, rewards, next_states, dones = \u001b[38;5;28mmap\u001b[39m(np.array, batch)\n\u001b[32m     73\u001b[39m states = torch.FloatTensor(states).to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:956\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (64, 5) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Paramètres\n",
    "num_episodes = 100\n",
    "max_steps = 1000\n",
    "batch_size = 64\n",
    "\n",
    "# Créer l’environnement\n",
    "env = SailingEnv(**get_initial_windfield(\"training_1\"))\n",
    "\n",
    "# Dimensions : observation directe, pas besoin de discrétiser\n",
    "example_obs, _ = env.reset(seed=0)\n",
    "state_dim = len(example_obs)\n",
    "\n",
    "# Créer l’agent DQN\n",
    "agent = DQNAgent(state_dim=state_dim)\n",
    "\n",
    "# Fixer les graines\n",
    "np.random.seed(42)\n",
    "agent.seed(42)\n",
    "\n",
    "# Historique\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "print(\"🚀 Starting DQN training with 100 episodes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset(seed=episode)\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = agent.act(obs)\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        agent.remember(obs, action, reward, next_obs, done)\n",
    "        agent.learn(batch_size=batch_size)\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step + 1)\n",
    "    success_history.append(done)\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        recent_success = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode + 1}: success rate (last 10) = {recent_success:.1f}%\")\n",
    "\n",
    "# Résumé\n",
    "training_time = time.time() - start_time\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\n✅ Training completed in {training_time:.1f}s\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Avg reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Avg steps: {np.mean(steps_history):.1f}\")\n",
    "\n",
    "# Sauvegarde\n",
    "agent.save(\"outputs/dqn_agent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec2518",
   "metadata": {},
   "source": [
    "### Visualizing Training Results\n",
    "\n",
    "Let's visualize how our agent improved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate rolling averages\n",
    "window_size = 10\n",
    "rolling_rewards = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_steps = np.convolve(steps_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_success = np.convolve([1 if s else 0 for s in success_history], np.ones(window_size)/window_size, mode='valid') * 100\n",
    "\n",
    "# Create the plots\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# Plot rewards\n",
    "ax1.plot(rolling_rewards)\n",
    "ax1.set_ylabel('Average Reward')\n",
    "ax1.set_title('Training Progress (10-episode rolling average)')\n",
    "\n",
    "# Plot steps\n",
    "ax2.plot(rolling_steps)\n",
    "ax2.set_ylabel('Average Steps')\n",
    "\n",
    "# Plot success rate\n",
    "#ax3.plot(rolling_success)\n",
    "#ax3.set_ylabel('Success Rate (%)')\n",
    "#ax3.set_xlabel('Episode')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c411f83",
   "metadata": {},
   "source": [
    "### Testing the Trained Agent\n",
    "\n",
    "Now let's evaluate our trained agent with exploration turned off to see how well it performs on unseen seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off exploration for evaluation\n",
    "ql_agent_full.exploration_rate = 0\n",
    "\n",
    "# Create test environment\n",
    "test_env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Test parameters\n",
    "num_test_episodes = 5\n",
    "max_steps = 1000\n",
    "\n",
    "print(\"Testing the trained agent on 5 new episodes...\")\n",
    "# Testing loop\n",
    "for episode in range(num_test_episodes):\n",
    "    # Reset environment\n",
    "    observation, info = test_env.reset(seed=1000 + episode)  # Different seeds from training\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action using learned policy\n",
    "        action = ql_agent_full.act(observation)\n",
    "        observation, reward, done, truncated, info = test_env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Test Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32c02f",
   "metadata": {},
   "source": [
    "## Visualizing Your Agent's Behavior\n",
    "\n",
    "## Save Your Agent For Submission\n",
    "\n",
    "## Training and Evaluating\n",
    "\n",
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883eaea8",
   "metadata": {},
   "source": [
    "# Next Steps for Developing Your Own Agent\n",
    "\n",
    "Now it's your turn to develop your own agent. Here are some suggestions:\n",
    "\n",
    "1. **Enhance the Q-Learning Agent**:\n",
    "   - Extend the state representation to incorporate the full wind field (not just local wind)\n",
    "   - This would allow the agent to anticipate wind changes and plan better routes\n",
    "   - Hint: Modify the `discretize_state` method to extract and process relevant features from the flattened wind field\n",
    "\n",
    "2. **Algorithmic Improvements**:\n",
    "   - Implement function approximation to handle continuous state spaces better\n",
    "   - Explore other RL algorithms like SARSA, Expected SARSA, or Deep Q-Networks\n",
    "   - Experiment with different exploration strategies that adapt over time\n",
    "\n",
    "3. **Physics-Based Approaches**:\n",
    "   - Leverage your understanding of sailing physics (from challenge_walkthrough notebook)\n",
    "   - Implement rule-based algorithms or path planning (A*, etc.) that take advantage of domain knowledge\n",
    "   - Create hybrid approaches that combine RL with domain-specific rules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
