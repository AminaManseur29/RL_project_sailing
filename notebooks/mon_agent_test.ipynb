{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e66641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the BaseAgent class\n",
    "from src.agents.mes_agents import MonAgent1, SARSAAgent, ExpectedSARSAAgent, DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8ac2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_sailing import SailingEnv\n",
    "from initial_windfields import get_initial_windfield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c368d",
   "metadata": {},
   "source": [
    "### Testing the Agent's Validity\n",
    "\n",
    "Let's make the agent do a few steps to check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4694e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of validating the agent here, we'll just demonstrate it on a simple task\n",
    "from src.env_sailing import SailingEnv\n",
    "\n",
    "# Create a simple environment\n",
    "env = SailingEnv()\n",
    "observation, info = env.reset(seed=42)\n",
    "state_dim = len(observation)\n",
    "\n",
    "# Initialize our agents\n",
    "mon_agent1 = MonAgent1()\n",
    "mon_agent.seed(42)\n",
    "\n",
    "sarsa_agent = SARSAAgent()\n",
    "sarsa_agent.seed(42)\n",
    "\n",
    "expectedsarsa_agent = ExpectedSARSAAgent()\n",
    "expectedsarsa_agent.seed(42)\n",
    "\n",
    "dqn_agent = DQNAgent(state_dim=state_dim)\n",
    "\n",
    "\n",
    "# Run the agent for a few steps\n",
    "print(\"Running the minimal agent for 5 steps:\")\n",
    "for i in range(5):\n",
    "    action = mon_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: Action={action}, Position={info['position']}, Reward={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d67fbb",
   "metadata": {},
   "source": [
    "### Analysis of the Agent\n",
    "\n",
    "The `MonAgent` is extremely simple but illustrates the key requirements for a valid agent:\n",
    "\n",
    "1. **Inheritance**: It inherits from `BaseAgent`\n",
    "2. **Required Methods**: It implements all required methods (`act`, `reset`, `seed`)\n",
    "3. **Action Selection**: It always returns action `0` (North)\n",
    "4. **Simplicity**: It maintains no internal state and requires no complex logic\n",
    "\n",
    "This agent provides a good baseline, but it has obvious limitations:\n",
    "\n",
    "- It ignores wind conditions completely\n",
    "- It will struggle when the wind is coming from the North\n",
    "- It doesn't adapt its strategy based on the environment\n",
    "\n",
    "Let's test the naive agent to see how well it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "mon_agent = MonAgent()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = mon_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6501b",
   "metadata": {},
   "source": [
    "## Improving on the Naive Agent\n",
    "\n",
    "The naive agent provides a good starting point, but there are many ways to improve it:\n",
    "\n",
    "1. **Wind-Aware Agent**: Consider wind direction when choosing actions\n",
    "2. **Goal-Directed Agent**: Calculate the direction to the goal and choose actions accordingly\n",
    "3. **Physics-Based Agent**: Use sailing physics equations to determine the most efficient action\n",
    "\n",
    "The key insight for sailing is that certain directions relative to the wind are more efficient than others:\n",
    "\n",
    "- The sailing efficiency is highest when moving perpendicular to the wind (beam reach)\n",
    "- It's difficult to sail directly into the wind (the \"no-go zone\" - less than 45Â° to the wind)\n",
    "- The boat maintains momentum (inertia) between steps\n",
    "\n",
    "Before diving into reinforcement learning, consider implementing a simple rule-based agent that incorporates these physics principles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276a2ed3",
   "metadata": {},
   "source": [
    "### Implementing a Q-Learning Agent\n",
    "\n",
    "Now let's implement a basic Q-learning agent for our sailing environment. Q-learning is a model-free reinforcement learning algorithm that learns to make decisions by estimating the value of state-action pairs.\n",
    "\n",
    "Our implementation will use a simplified state representation based on:\n",
    "1. Agent's current position\n",
    "2. Agent's current velocity \n",
    "3. Local wind at the agent's position\n",
    "\n",
    "This simplified approach makes the agent more interpretable and faster to train, while still capturing essential local information for effective navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.mon_agent import QLearningAgent, MonAgent1, SARSAAgent, ExpectedSARSAAgent, DQNAgent, MonAgent5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af3d32",
   "metadata": {},
   "source": [
    "### Training the Q-Learning Agent\n",
    "\n",
    "Now let's train our Q-learning agent on a simple initial windfield. We'll start with a small number of episodes (10) to demonstrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa07c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Q-learning agent\n",
    "ql_agent = QLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.2)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10  # Small number for debugging\n",
    "max_steps = 1000\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training with 10 episodes (debug run)...\")\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")\n",
    "    \n",
    "    # Update exploration rate (optional: decrease exploration over time)\n",
    "    ql_agent.exploration_rate = max(0.05, ql_agent.exploration_rate * 0.95)\n",
    "\n",
    "print(\"\\nDebug training completed!\")\n",
    "print(f\"Q-table size: {len(ql_agent.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9c9e9",
   "metadata": {},
   "source": [
    "### Full Training Run\n",
    "\n",
    "Now let's train our agent for more episodes to get better performance. This will take longer but should result in a more effective agent.\n",
    "\n",
    "*Note: You might want to adjust the number of episodes based on your available time. More episodes generally lead to better performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3dc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Q-learning agent for full training\n",
    "ql_agent_full = QLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent_full.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 100  # More episodes for better learning\n",
    "max_steps = 1000\n",
    "\n",
    "# Progress tracking\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting full training with 100 episodes...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent_full.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent_full.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent_full.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent_full.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Record metrics\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step+1)\n",
    "    success_history.append(done)\n",
    "    \n",
    "    # Update exploration rate (decrease over time)\n",
    "    ql_agent_full.exploration_rate = max(0.05, ql_agent_full.exploration_rate * 0.98)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode+1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(ql_agent_full.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4ff19",
   "metadata": {},
   "source": [
    "### EntraÃ®nement Agent1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94cb65",
   "metadata": {},
   "source": [
    "### EntraÃ®nement de SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b527dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er l'agent SARSA\n",
    "sarsa_agent = SARSAAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Fixer la graine pour reproductibilitÃ©\n",
    "np.random.seed(42)\n",
    "sarsa_agent.seed(42)\n",
    "\n",
    "# Environnement\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# ParamÃ¨tres\n",
    "num_episodes = 100\n",
    "max_steps = 1000\n",
    "\n",
    "# Historique\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "print(\"Starting SARSA training with 100 episodes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset(seed=episode)\n",
    "    state = sarsa_agent.discretize_state(observation)\n",
    "    action = sarsa_agent.act(observation)\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = sarsa_agent.discretize_state(next_observation)\n",
    "        next_action = sarsa_agent.act(next_observation)\n",
    "\n",
    "        # Mise Ã  jour SARSA\n",
    "        sarsa_agent.learn(state, action, reward, next_state, next_action)\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step + 1)\n",
    "    success_history.append(done)\n",
    "\n",
    "    # Diminuer l'exploration progressivement\n",
    "    sarsa_agent.exploration_rate = max(0.05, sarsa_agent.exploration_rate * 0.98)\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode + 1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(sarsa_agent.q_table)} states\")\n",
    "\n",
    "# Sauvegarde du modÃ¨le\n",
    "sarsa_agent.save(\"outputs/sarsa_agent.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa2c0c",
   "metadata": {},
   "source": [
    "### EntraÃ®nement Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er l'agent Expected SARSA\n",
    "expected_agent = ExpectedSARSAAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Fixer la graine\n",
    "np.random.seed(42)\n",
    "expected_agent.seed(42)\n",
    "\n",
    "# Environnement\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# ParamÃ¨tres d'entraÃ®nement\n",
    "num_episodes = 100\n",
    "max_steps = 1000\n",
    "\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "print(\"Starting Expected SARSA training with 100 episodes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset(seed=episode)\n",
    "    state = expected_agent.discretize_state(observation)\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = expected_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = expected_agent.discretize_state(next_observation)\n",
    "\n",
    "        expected_agent.learn(state, action, reward, next_state)\n",
    "\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step + 1)\n",
    "    success_history.append(done)\n",
    "\n",
    "    # Diminution progressive d'epsilon\n",
    "    expected_agent.exploration_rate = max(0.05, expected_agent.exploration_rate * 0.98)\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode + 1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(expected_agent.q_table)} states\")\n",
    "\n",
    "# Sauvegarde\n",
    "expected_agent.save(\"outputs/expected_sarsa_agent.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23479d5e",
   "metadata": {},
   "source": [
    "### EntraÃ®nement Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e82e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParamÃ¨tres\n",
    "num_episodes = 100\n",
    "max_steps = 1000\n",
    "batch_size = 64\n",
    "\n",
    "# CrÃ©er lâ€™environnement\n",
    "env = SailingEnv(**get_initial_windfield(\"training_1\"))\n",
    "\n",
    "# Dimensions : observation directe, pas besoin de discrÃ©tiser\n",
    "example_obs, _ = env.reset(seed=0)\n",
    "state_dim = len(example_obs)\n",
    "\n",
    "# CrÃ©er lâ€™agent DQN\n",
    "agent = DQNAgent(state_dim=state_dim)\n",
    "\n",
    "# Fixer les graines\n",
    "np.random.seed(42)\n",
    "agent.seed(42)\n",
    "\n",
    "# Historique\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "print(\"ðŸš€ Starting DQN training with 100 episodes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset(seed=episode)\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = agent.act(obs)\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        agent.remember(obs, action, reward, next_obs, done)\n",
    "        agent.learn(batch_size=batch_size)\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step + 1)\n",
    "    success_history.append(done)\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        recent_success = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode + 1}: success rate (last 10) = {recent_success:.1f}%\")\n",
    "\n",
    "# RÃ©sumÃ©\n",
    "training_time = time.time() - start_time\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nâœ… Training completed in {training_time:.1f}s\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Avg reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Avg steps: {np.mean(steps_history):.1f}\")\n",
    "\n",
    "# Sauvegarde\n",
    "agent.save(\"outputs/dqn_agent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec2518",
   "metadata": {},
   "source": [
    "### Visualizing Training Results\n",
    "\n",
    "Let's visualize how our agent improved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate rolling averages\n",
    "window_size = 10\n",
    "rolling_rewards = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_steps = np.convolve(steps_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_success = np.convolve([1 if s else 0 for s in success_history], np.ones(window_size)/window_size, mode='valid') * 100\n",
    "\n",
    "# Create the plots\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# Plot rewards\n",
    "ax1.plot(rolling_rewards)\n",
    "ax1.set_ylabel('Average Reward')\n",
    "ax1.set_title('Training Progress (10-episode rolling average)')\n",
    "\n",
    "# Plot steps\n",
    "ax2.plot(rolling_steps)\n",
    "ax2.set_ylabel('Average Steps')\n",
    "\n",
    "# Plot success rate\n",
    "#ax3.plot(rolling_success)\n",
    "#ax3.set_ylabel('Success Rate (%)')\n",
    "#ax3.set_xlabel('Episode')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c411f83",
   "metadata": {},
   "source": [
    "### Testing the Trained Agent\n",
    "\n",
    "Now let's evaluate our trained agent with exploration turned off to see how well it performs on unseen seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off exploration for evaluation\n",
    "ql_agent_full.exploration_rate = 0\n",
    "\n",
    "# Create test environment\n",
    "test_env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Test parameters\n",
    "num_test_episodes = 5\n",
    "max_steps = 1000\n",
    "\n",
    "print(\"Testing the trained agent on 5 new episodes...\")\n",
    "# Testing loop\n",
    "for episode in range(num_test_episodes):\n",
    "    # Reset environment\n",
    "    observation, info = test_env.reset(seed=1000 + episode)  # Different seeds from training\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action using learned policy\n",
    "        action = ql_agent_full.act(observation)\n",
    "        observation, reward, done, truncated, info = test_env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Test Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6a7d8",
   "metadata": {},
   "source": [
    "## Validating Your Agent\n",
    "\n",
    "After creating your agent, you'll want to ensure it meets all the requirements of the challenge. There are two ways to validate your agent:\n",
    "\n",
    "1. **Using the `validate_agent.ipynb` notebook:**\n",
    "   - This notebook provides a comprehensive interface for testing your agent\n",
    "   - It shows detailed validation results and explains any issues\n",
    "\n",
    "2. **Using the command line:**\n",
    "   ```bash\n",
    "   cd src\n",
    "   python test_agent_validity.py path/to/your_agent.py\n",
    "   ```\n",
    "\n",
    "We recommend using these tools after you've completed your agent implementation rather than trying to validate it during development.\n",
    "\n",
    "For now, let's focus on understanding agent design principles and implementing effective strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32c02f",
   "metadata": {},
   "source": [
    "## Visualizing Your Agent's Behavior\n",
    "\n",
    "## Save Your Agent For Submission\n",
    "\n",
    "## Training and Evaluating\n",
    "\n",
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883eaea8",
   "metadata": {},
   "source": [
    "# Next Steps for Developing Your Own Agent\n",
    "\n",
    "Now it's your turn to develop your own agent. Here are some suggestions:\n",
    "\n",
    "1. **Enhance the Q-Learning Agent**:\n",
    "   - Extend the state representation to incorporate the full wind field (not just local wind)\n",
    "   - This would allow the agent to anticipate wind changes and plan better routes\n",
    "   - Hint: Modify the `discretize_state` method to extract and process relevant features from the flattened wind field\n",
    "\n",
    "2. **Algorithmic Improvements**:\n",
    "   - Implement function approximation to handle continuous state spaces better\n",
    "   - Explore other RL algorithms like SARSA, Expected SARSA, or Deep Q-Networks\n",
    "   - Experiment with different exploration strategies that adapt over time\n",
    "\n",
    "3. **Physics-Based Approaches**:\n",
    "   - Leverage your understanding of sailing physics (from challenge_walkthrough notebook)\n",
    "   - Implement rule-based algorithms or path planning (A*, etc.) that take advantage of domain knowledge\n",
    "   - Create hybrid approaches that combine RL with domain-specific rules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
