{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Agents for the Sailing Challenge\n",
    "\n",
    "In this notebook, we'll explore how to design and implement agents for the Sailing Challenge. We'll cover:\n",
    "\n",
    "1. The requirements and interface for valid agents\n",
    "2. Understanding the greedy agent example \n",
    "3. Implementing a simple reinforcement learning agent\n",
    "\n",
    "By the end of this notebook, you'll have a clear understanding of how to create your own agents that can navigate the sailing environment effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Requirements\n",
    "\n",
    "All agents in the Sailing Challenge must implement a specific interface defined by the `BaseAgent` abstract class. Let's examine this class to understand what's required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the BaseAgent class\n",
    "from src.agents.base_agent import BaseAgent\n",
    "\n",
    "# Display the BaseAgent class documentation\n",
    "#help(BaseAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Methods\n",
    "\n",
    "As we can see from the `BaseAgent` class, any valid agent must implement:\n",
    "\n",
    "1. **`act(observation)`**: The core decision-making method that takes the current observation and returns an action\n",
    "   - Input: A numpy array containing [x, y, vx, vy, wx, wy, ...] representing position, velocity, wind, and the full wind field\n",
    "   - Output: An integer in the range [0-8] representing the action to take\n",
    "\n",
    "2. **`reset()`**: Resets the agent's internal state at the beginning of each episode\n",
    "   - This is particularly important for agents that maintain memory or state across steps\n",
    "\n",
    "3. **`seed(seed)`**: Sets the random seed for the agent to ensure reproducibility\n",
    "   - This is crucial for evaluation and comparison of different agents\n",
    "\n",
    "Additionally, while not strictly required, implementing `save()` and `load()` methods is recommended for storing and retrieving trained agent parameters.\n",
    "\n",
    "### The Validation Process\n",
    "\n",
    "When you submit an agent, it will be automatically validated against these requirements. The validation process checks:\n",
    "\n",
    "1. If the agent class inherits from `BaseAgent`\n",
    "2. If all required methods are implemented with correct parameters\n",
    "3. If the agent produces valid actions (integers in range [0-8])\n",
    "4. If the agent can interact with the environment without errors\n",
    "\n",
    "Let's create a minimal valid agent to understand this process better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalAgent(BaseAgent):\n",
    "    \"\"\"A minimal valid agent that meets all interface requirements.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"Choose an action randomly.\"\"\"\n",
    "        return self.np_random.integers(0, 9)  # Random action from 0-8\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the agent.\"\"\"\n",
    "        pass  # Nothing to reset in this simple agent\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "# Create an instance of our minimal agent\n",
    "minimal_agent = MinimalAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Agent's Validity\n",
    "\n",
    "Let's make the agent do a few steps to check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the minimal agent for 5 steps:\n",
      "Step 1: Action=0, Position=[16  1], Reward=0.0\n",
      "Step 2: Action=6, Position=[15  1], Reward=0.0\n",
      "Step 3: Action=5, Position=[14  0], Reward=0.0\n",
      "Step 4: Action=3, Position=[15  0], Reward=0.0\n",
      "Step 5: Action=3, Position=[16  0], Reward=0.0\n"
     ]
    }
   ],
   "source": [
    "# Instead of validating the agent here, we'll just demonstrate it on a simple task\n",
    "from src.env_sailing import SailingEnv\n",
    "\n",
    "# Create a simple environment\n",
    "env = SailingEnv()\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Initialize our minimal agent\n",
    "minimal_agent = MinimalAgent()\n",
    "minimal_agent.seed(42)\n",
    "\n",
    "# Run the agent for a few steps\n",
    "print(\"Running the minimal agent for 5 steps:\")\n",
    "for i in range(5):\n",
    "    action = minimal_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: Action={action}, Position={info['position']}, Reward={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Your Agent\n",
    "\n",
    "After creating your agent, you'll want to ensure it meets all the requirements of the challenge. There are two ways to validate your agent:\n",
    "\n",
    "1. **Using the `validate_agent.ipynb` notebook:**\n",
    "   - This notebook provides a comprehensive interface for testing your agent\n",
    "   - It shows detailed validation results and explains any issues\n",
    "\n",
    "2. **Using the command line:**\n",
    "   ```bash\n",
    "   cd src\n",
    "   python test_agent_validity.py path/to/your_agent.py\n",
    "   ```\n",
    "\n",
    "We recommend using these tools after you've completed your agent implementation rather than trying to validate it during development.\n",
    "\n",
    "For now, let's focus on understanding agent design principles and implementing effective strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Observation and Action Space\n",
    "\n",
    "To design effective agents, it's important to understand:\n",
    "\n",
    "1. **What information is available to the agent (observations)**\n",
    "2. **What actions the agent can take**\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The observation provided to your agent is a numpy array with the following structure:\n",
    "\n",
    "`[x, y, vx, vy, wx, wy, flattened_wind_field]`\n",
    "\n",
    "\n",
    "Where:\n",
    "- `x, y`: Current position (grid coordinates)\n",
    "- `vx, vy`: Current velocity vector \n",
    "- `wx, wy`: Wind vector at the current position\n",
    "- `flattened_wind_field`: The entire wind field (can be reshaped to grid_size × grid_size × 2)\n",
    "\n",
    "For simpler agents, you might only need to use the first 6 values. More sophisticated agents can use the full wind field to plan ahead.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The agent can choose from 9 possible actions:\n",
    "\n",
    "- 0: Move North (up)\n",
    "- 1: Move Northeast\n",
    "- 2: Move East (right)\n",
    "- 3: Move Southeast\n",
    "- 4: Move South (down)\n",
    "- 5: Move Southwest\n",
    "- 6: Move West (left)\n",
    "- 7: Move Northwest\n",
    "- 8: Stay in place\n",
    "\n",
    "Each action represents a desired direction for the boat to move. However, the actual movement will be influenced by the wind and sailing physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Agent Example\n",
    "\n",
    "Let's examine the built-in `NaiveAgent`, which provides a simple baseline implementation. This agent always tries to move North (toward the goal), regardless of wind conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class NaiveAgent(BaseAgent):\n",
      "    \"\"\"\n",
      "    A naive agent for the Sailing Challenge.\n",
      "\n",
      "    This is a very simple agent that always chooses to go North,\n",
      "    regardless of wind conditions or position. It serves as a minimal\n",
      "    working example that students can build upon.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the agent.\"\"\"\n",
      "        super().__init__()\n",
      "        self.np_random = np.random.default_rng()\n",
      "\n",
      "    def act(self, observation: np.ndarray) -> int:\n",
      "        \"\"\"\n",
      "        Select an action based on the current observation.\n",
      "\n",
      "        Args:\n",
      "            observation: A numpy array containing the current observation.\n",
      "                Format: [x, y, vx, vy, wx, wy] where:\n",
      "                - (x, y) is the current position\n",
      "                - (vx, vy) is the current velocity\n",
      "                - (wx, wy) is the current wind vector\n",
      "\n",
      "        Returns:\n",
      "            action: An integer in [0, 8] representing the action to take:\n",
      "                - 0: Move North\n",
      "                - 1: Move Northeast\n",
      "                - 2: Move East\n",
      "                - 3: Move Southeast\n",
      "                - 4: Move South\n",
      "                - 5: Move Southwest\n",
      "                - 6: Move West\n",
      "                - 7: Move Northwest\n",
      "                - 8: Stay in place\n",
      "        \"\"\"\n",
      "        # This agent always chooses to go North (action 0)\n",
      "        return 0\n",
      "\n",
      "    def reset(self) -> None:\n",
      "        \"\"\"Reset the agent's internal state between episodes.\"\"\"\n",
      "        # Nothing to reset for this simple agent\n",
      "        pass\n",
      "\n",
      "    def seed(self, seed: int = None) -> None:\n",
      "        \"\"\"Set the random seed for reproducibility.\"\"\"\n",
      "        self.np_random = np.random.default_rng(seed)\n",
      "\n",
      "    def save(self, path: str) -> None:\n",
      "        \"\"\"\n",
      "        Save the agent's learned parameters to a file.\n",
      "\n",
      "        Args:\n",
      "            path: Path to save the agent's state\n",
      "        \"\"\"\n",
      "        # No parameters to save for this simple agent\n",
      "        pass\n",
      "\n",
      "    def load(self, path: str) -> None:\n",
      "        \"\"\"\n",
      "        Load the agent's learned parameters from a file.\n",
      "        Args:\n",
      "            path: Path to load the agent's state from\n",
      "        \"\"\"\n",
      "        # No parameters to load for this simple agent\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the NaiveAgent\n",
    "from src.agents.agent_naive import NaiveAgent\n",
    "\n",
    "# Display the source code\n",
    "import inspect\n",
    "print(inspect.getsource(NaiveAgent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Naive Agent\n",
    "\n",
    "The `NaiveAgent` is extremely simple but illustrates the key requirements for a valid agent:\n",
    "\n",
    "1. **Inheritance**: It inherits from `BaseAgent`\n",
    "2. **Required Methods**: It implements all required methods (`act`, `reset`, `seed`)\n",
    "3. **Action Selection**: It always returns action `0` (North)\n",
    "4. **Simplicity**: It maintains no internal state and requires no complex logic\n",
    "\n",
    "This agent provides a good baseline, but it has obvious limitations:\n",
    "\n",
    "- It ignores wind conditions completely\n",
    "- It will struggle when the wind is coming from the North\n",
    "- It doesn't adapt its strategy based on the environment\n",
    "\n",
    "Let's test the naive agent to see how well it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the naive agent on the simple_static initial windfield:\n",
      "Step 10: Position=[16  7], Reward=0.0\n",
      "Step 20: Position=[16 12], Reward=0.0\n",
      "Step 30: Position=[16 13], Reward=0.0\n",
      "Step 40: Position=[16 13], Reward=0.0\n",
      "Step 50: Position=[16 14], Reward=0.0\n",
      "Step 60: Position=[16 14], Reward=0.0\n",
      "Step 70: Position=[16 15], Reward=0.0\n",
      "Step 80: Position=[16 15], Reward=0.0\n",
      "Step 90: Position=[16 16], Reward=0.0\n",
      "Step 100: Position=[16 16], Reward=0.0\n",
      "Step 110: Position=[16 17], Reward=0.0\n",
      "Step 120: Position=[16 18], Reward=0.0\n",
      "Step 130: Position=[16 18], Reward=0.0\n",
      "Step 140: Position=[16 19], Reward=0.0\n",
      "Step 150: Position=[16 19], Reward=0.0\n",
      "Step 160: Position=[16 20], Reward=0.0\n",
      "Step 170: Position=[16 20], Reward=0.0\n",
      "Step 180: Position=[16 21], Reward=0.0\n",
      "Step 190: Position=[16 21], Reward=0.0\n",
      "Step 200: Position=[16 22], Reward=0.0\n",
      "Step 210: Position=[16 22], Reward=0.0\n",
      "Step 220: Position=[16 23], Reward=0.0\n",
      "Step 230: Position=[16 24], Reward=0.0\n",
      "Step 240: Position=[16 24], Reward=0.0\n",
      "Step 250: Position=[16 25], Reward=0.0\n",
      "Step 260: Position=[16 25], Reward=0.0\n",
      "Step 270: Position=[16 26], Reward=0.0\n",
      "Step 280: Position=[16 27], Reward=0.0\n",
      "Step 290: Position=[16 27], Reward=0.0\n",
      "Step 300: Position=[16 29], Reward=0.0\n",
      "\n",
      "Episode finished after 301 steps with reward: 100.0\n",
      "Final position: [16 30]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "naive_agent = NaiveAgent()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = naive_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving on the Naive Agent\n",
    "\n",
    "The naive agent provides a good starting point, but there are many ways to improve it:\n",
    "\n",
    "1. **Wind-Aware Agent**: Consider wind direction when choosing actions\n",
    "2. **Goal-Directed Agent**: Calculate the direction to the goal and choose actions accordingly\n",
    "3. **Physics-Based Agent**: Use sailing physics equations to determine the most efficient action\n",
    "\n",
    "The key insight for sailing is that certain directions relative to the wind are more efficient than others:\n",
    "\n",
    "- The sailing efficiency is highest when moving perpendicular to the wind (beam reach)\n",
    "- It's difficult to sail directly into the wind (the \"no-go zone\" - less than 45° to the wind)\n",
    "- The boat maintains momentum (inertia) between steps\n",
    "\n",
    "Before diving into reinforcement learning, consider implementing a simple rule-based agent that incorporates these physics principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.sailing_smart_agent import SailingSmartAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the SailingSmartAgent on the simple_static initial windfield:\n",
      "Step 10: Position=[16  7], Reward=0.0\n",
      "Step 20: Position=[16 12], Reward=0.0\n",
      "Step 30: Position=[16 13], Reward=0.0\n",
      "Step 40: Position=[16 13], Reward=0.0\n",
      "Step 50: Position=[16 14], Reward=0.0\n",
      "Step 60: Position=[16 14], Reward=0.0\n",
      "Step 70: Position=[16 15], Reward=0.0\n",
      "Step 80: Position=[16 15], Reward=0.0\n",
      "Step 90: Position=[16 16], Reward=0.0\n",
      "Step 100: Position=[16 16], Reward=0.0\n",
      "Step 110: Position=[16 17], Reward=0.0\n",
      "Step 120: Position=[16 18], Reward=0.0\n",
      "Step 130: Position=[16 18], Reward=0.0\n",
      "Step 140: Position=[16 19], Reward=0.0\n",
      "Step 150: Position=[16 19], Reward=0.0\n",
      "Step 160: Position=[16 20], Reward=0.0\n",
      "Step 170: Position=[16 20], Reward=0.0\n",
      "Step 180: Position=[16 21], Reward=0.0\n",
      "Step 190: Position=[16 21], Reward=0.0\n",
      "Step 200: Position=[16 22], Reward=0.0\n",
      "Step 210: Position=[16 22], Reward=0.0\n",
      "Step 220: Position=[16 23], Reward=0.0\n",
      "Step 230: Position=[16 24], Reward=0.0\n",
      "Step 240: Position=[16 24], Reward=0.0\n",
      "Step 250: Position=[16 25], Reward=0.0\n",
      "Step 260: Position=[16 25], Reward=0.0\n",
      "Step 270: Position=[16 26], Reward=0.0\n",
      "Step 280: Position=[16 27], Reward=0.0\n",
      "Step 290: Position=[16 27], Reward=0.0\n",
      "Step 300: Position=[16 29], Reward=0.0\n",
      "\n",
      "Episode finished after 301 steps with total reward: 100.00\n",
      "Final position: [16 30]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "# Crée l’environnement avec un champ de vent simple (par ex. 'simple_static')\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "smart_agent = SailingSmartAgent()\n",
    "\n",
    "# Paramètres de l’épisode\n",
    "max_steps = 1000\n",
    "seed = 42\n",
    "\n",
    "# Initialisation\n",
    "observation, info = env.reset(seed=seed)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the SailingSmartAgent on the simple_static initial windfield:\")\n",
    "\n",
    "# Boucle principale\n",
    "while not (done or truncated) and step_count < max_steps:\n",
    "    action = smart_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "\n",
    "    # Affichage toutes les 10 étapes\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "# Résumé final\n",
    "print(\"\\nEpisode finished after {} steps with total reward: {:.2f}\".format(step_count, total_reward))\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Simple RL Agent\n",
    "\n",
    "## Implementing a Q-Learning Agent\n",
    "\n",
    "Now let's implement a basic Q-learning agent for our sailing environment. Q-learning is a model-free reinforcement learning algorithm that learns to make decisions by estimating the value of state-action pairs.\n",
    "\n",
    "Our implementation will use a simplified state representation based on:\n",
    "1. Agent's current position\n",
    "2. Agent's current velocity \n",
    "3. Local wind at the agent's position\n",
    "\n",
    "This simplified approach makes the agent more interpretable and faster to train, while still capturing essential local information for effective navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(BaseAgent):\n",
    "    \"\"\"A simple Q-learning agent for the sailing environment using only local information.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        \n",
    "        # State discretization parameters\n",
    "        self.position_bins = 8     # Discretize the grid into 8x8\n",
    "        self.velocity_bins = 4     # Discretize velocity into 4 bins\n",
    "        self.wind_bins = 8         # Discretize wind directions into 8 bins\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        # State space: position_x, position_y, velocity_direction, wind_direction\n",
    "        # Action space: 9 possible actions\n",
    "        self.q_table = {}\n",
    "        \n",
    "    def discretize_state(self, observation):\n",
    "        \"\"\"Convert continuous observation to discrete state for Q-table lookup.\"\"\"\n",
    "        # Extract position, velocity and wind from observation\n",
    "        x, y = observation[0], observation[1]\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Discretize position (assume 32x32 grid)\n",
    "        grid_size = 32\n",
    "        x_bin = min(int(x / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        y_bin = min(int(y / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        \n",
    "        # Discretize velocity direction (ignoring magnitude for simplicity)\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        if v_magnitude < 0.1:  # If velocity is very small, consider it as a separate bin\n",
    "            v_bin = 0\n",
    "        else:\n",
    "            v_direction = np.arctan2(vy, vx)  # Range: [-pi, pi]\n",
    "            v_bin = int(((v_direction + np.pi) / (2 * np.pi) * (self.velocity_bins-1)) + 1) % self.velocity_bins\n",
    "        \n",
    "        # Discretize wind direction\n",
    "        wind_direction = np.arctan2(wy, wx)  # Range: [-pi, pi]\n",
    "        wind_bin = int(((wind_direction + np.pi) / (2 * np.pi) * self.wind_bins)) % self.wind_bins\n",
    "        \n",
    "        # Return discrete state tuple\n",
    "        return (x_bin, y_bin, v_bin, wind_bin)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
    "        # Discretize the state\n",
    "        state = self.discretize_state(observation)\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if self.np_random.random() < self.exploration_rate:\n",
    "            # Explore: choose a random action\n",
    "            return self.np_random.integers(0, 9)\n",
    "        else:\n",
    "            # Exploit: choose the best action according to Q-table\n",
    "            if state not in self.q_table:\n",
    "                # If state not in Q-table, initialize it\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            \n",
    "            # Return action with highest Q-value\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-table based on observed transition.\"\"\"\n",
    "        # Initialize Q-values if states not in table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(9)\n",
    "        \n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent for a new episode.\"\"\"\n",
    "        # Nothing to reset for Q-learning agent\n",
    "        pass\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the Q-table to a file.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.q_table, f)\n",
    "            \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the Q-table from a file.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'rb') as f:\n",
    "            self.q_table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Q-Learning Agent\n",
    "\n",
    "Now let's train our Q-learning agent on a simple initial windfield. We'll start with a small number of episodes (10) to demonstrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.good_agents.hybrid_smart_qlearning_agent import ExpectedSARSALambdaSmartAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 10 episodes (debug run)...\n",
      "Episode 1: Steps=1000, Reward=0.0, Position=[ 8 30], Goal reached=True\n",
      "Episode 2: Steps=1000, Reward=0.0, Position=[ 1 30], Goal reached=True\n",
      "Episode 3: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 4: Steps=1000, Reward=0.0, Position=[ 1 24], Goal reached=True\n",
      "Episode 5: Steps=1000, Reward=0.0, Position=[ 4 30], Goal reached=True\n",
      "Episode 6: Steps=1000, Reward=0.0, Position=[ 0 27], Goal reached=True\n",
      "Episode 7: Steps=1000, Reward=0.0, Position=[ 4 31], Goal reached=True\n",
      "Episode 8: Steps=1000, Reward=0.0, Position=[ 7 29], Goal reached=True\n",
      "Episode 9: Steps=1000, Reward=0.0, Position=[ 1 31], Goal reached=True\n",
      "Episode 10: Steps=716, Reward=100.0, Position=[15 30], Goal reached=True\n",
      "\n",
      "Debug training completed!\n",
      "Q-table size: 215 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent\n",
    "ql_agent = QLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.2)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10  # Small number for debugging\n",
    "max_steps = 1000\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training with 10 episodes (debug run)...\")\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")\n",
    "    \n",
    "    # Update exploration rate (optional: decrease exploration over time)\n",
    "    ql_agent.exploration_rate = max(0.05, ql_agent.exploration_rate * 0.95)\n",
    "\n",
    "print(\"\\nDebug training completed!\")\n",
    "print(f\"Q-table size: {len(ql_agent.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training Run\n",
    "\n",
    "Now let's train our agent for more episodes to get better performance. This will take longer but should result in a more effective agent.\n",
    "\n",
    "*Note: You might want to adjust the number of episodes based on your available time. More episodes generally lead to better performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Reward shaping basé sur le progrès vers l'objectif\n",
    "def custom_reward(obs, next_obs, reward, done, goal):\n",
    "    if done:\n",
    "        return reward\n",
    "    pos = obs[:2]\n",
    "    next_pos = next_obs[:2]\n",
    "    move = next_pos - pos\n",
    "    goal_vec = goal - pos\n",
    "\n",
    "    if np.linalg.norm(move) < 1e-2:\n",
    "        return -1  # Pénalité si immobilité\n",
    "\n",
    "    progress = np.dot(move, goal_vec) / (np.linalg.norm(move) * np.linalg.norm(goal_vec) + 1e-8)\n",
    "    return progress * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_sailing import SailingEnv\n",
    "from initial_windfields import get_initial_windfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training ExpectedSARSALambdaSmartAgent for 500 episodes...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SmartSARSAAgent.learn() missing 1 required positional argument: 'next_action'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m next_action = agent.act(next_obs)\n\u001b[32m     39\u001b[39m shaped_reward = custom_reward(obs, next_obs, reward, done, np.array([\u001b[32m17\u001b[39m, \u001b[32m31\u001b[39m]))\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshaped_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m obs = next_obs\n\u001b[32m     43\u001b[39m state = next_state\n",
      "\u001b[31mTypeError\u001b[39m: SmartSARSAAgent.learn() missing 1 required positional argument: 'next_action'"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent for full training\n",
    "agent = SmartSARSAAgent()\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "agent.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 500  # More episodes for better learning\n",
    "max_steps = 1000\n",
    "\n",
    "epsilon_decay = 0.98\n",
    "min_epsilon = 0.05\n",
    "\n",
    "# Progress tracking\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "print(f\"🚀 Training ExpectedSARSALambdaSmartAgent for {num_episodes} episodes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    agent.set_goal([17, 31])\n",
    "    agent.reset()  # Reset eligibility traces\n",
    "    state = agent.discretize_state(obs)\n",
    "    action = agent.act(obs)\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        next_state = agent.discretize_state(next_obs)\n",
    "        next_action = agent.act(next_obs)\n",
    "\n",
    "        shaped_reward = custom_reward(obs, next_obs, reward, done, np.array([17, 31]))\n",
    "        agent.learn(state, action, shaped_reward, next_state)\n",
    "\n",
    "        obs = next_obs\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        total_reward += reward\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step + 1)\n",
    "    success_history.append(done)\n",
    "\n",
    "    # Diminution de epsilon\n",
    "    agent.epsilon = max(min_epsilon, agent.epsilon * epsilon_decay)\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        recent_success = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Success rate (last 10) = {recent_success:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\n✅ Training completed in {training_time:.1f} seconds\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(agent.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.new_agent_4 import SmartSARSAAgentWithSailingRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of validating the agent here, we'll just demonstrate it on a simple task\n",
    "from src.env_sailing import SailingEnv\n",
    "\n",
    "# Create a simple environment\n",
    "env = SailingEnv()\n",
    "observation, info = env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_sarsa(agent, env, num_episodes=100, max_steps=1000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    agent.seed(seed)\n",
    "\n",
    "    rewards_history = []\n",
    "    steps_history = []\n",
    "    success_history = []\n",
    "\n",
    "    print(f\"Starting SARSA training for {num_episodes} episodes...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        observation, info = env.reset(seed=episode)\n",
    "        state = agent.discretize_state(observation) if hasattr(agent, 'discretize_state') else observation\n",
    "        \n",
    "        action = agent.act(observation)  # action initiale (pour SARSA)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            next_observation, reward, done, truncated, info = env.step(action)\n",
    "            next_state = agent.discretize_state(next_observation) if hasattr(agent, 'discretize_state') else next_observation\n",
    "            \n",
    "            next_action = agent.act(next_observation)  # action suivante (SARSA)\n",
    "            \n",
    "            # Apprentissage avec state, action, reward, next_state, next_action\n",
    "            agent.learn(state, action, reward, next_state, next_action)\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            observation = next_observation\n",
    "            total_reward += reward\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "        steps_history.append(step + 1)\n",
    "        success_history.append(done)\n",
    "\n",
    "        # Décroissance du taux d'exploration si agent a cet attribut\n",
    "        if hasattr(agent, 'exploration_rate'):\n",
    "            agent.exploration_rate = max(0.05, agent.exploration_rate * 0.98)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            recent_success = sum(success_history[-10:]) / 10 * 100\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}: Success rate (last 10): {recent_success:.1f}%\")\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    overall_success = sum(success_history) / len(success_history) * 100\n",
    "    print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "    print(f\"Overall success rate: {overall_success:.1f}%\")\n",
    "    print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "    print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "\n",
    "    if hasattr(agent, 'save'):\n",
    "        agent.save(\"outputs/trained_agent_sarsa.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from env_sailing import SailingEnv\n",
    "from initial_windfields import get_initial_windfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SARSA training for 100 episodes...\n",
      "Episode 10/100: Success rate (last 10): 100.0%\n",
      "Episode 20/100: Success rate (last 10): 100.0%\n",
      "Episode 30/100: Success rate (last 10): 100.0%\n",
      "Episode 40/100: Success rate (last 10): 100.0%\n",
      "Episode 50/100: Success rate (last 10): 100.0%\n",
      "Episode 60/100: Success rate (last 10): 100.0%\n",
      "Episode 70/100: Success rate (last 10): 100.0%\n",
      "Episode 80/100: Success rate (last 10): 100.0%\n",
      "Episode 90/100: Success rate (last 10): 100.0%\n",
      "Episode 100/100: Success rate (last 10): 100.0%\n",
      "\n",
      "Training completed in 3.3 seconds!\n",
      "Overall success rate: 100.0%\n",
      "Average reward: 100.00\n",
      "Average steps: 81.6\n"
     ]
    }
   ],
   "source": [
    "mon_agent = SmartSARSAAgentWithSailingRules()\n",
    "\n",
    "np.random.seed(42)\n",
    "mon_agent.seed(42)\n",
    "# Training parameters\n",
    "num_episodes = 100  # More episodes for better learning\n",
    "max_steps = 1000\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "train_agent_sarsa(mon_agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Extensions to the Q-Learning Agent\n",
    "\n",
    "This simplified Q-learning implementation provides a good starting point but has several limitations:\n",
    "\n",
    "1. **Limited State Representation**: It only uses local information (position, velocity, and local wind) without considering the full wind field, which limits the agent's ability to plan ahead.\n",
    "\n",
    "2. **Discrete State Space**: The discretization loses information and may not capture subtle differences in states.\n",
    "\n",
    "3. **Fixed Exploration Rate**: The exploration rate doesn't adapt based on learning progress.\n",
    "\n",
    "#### How to Extend the Agent:\n",
    "\n",
    "1. **Incorporating the Full Wind Field**:\n",
    "   - You could extend the state representation to include information from the full wind field (observation indices 6 onward).\n",
    "   - Create a more sophisticated discretization that captures wind patterns relevant to planning.\n",
    "   - Example approach: Sample key grid points ahead of the boat's position or in the direction of the goal.\n",
    "\n",
    "2. **Function Approximation**:\n",
    "   - Replace the discrete Q-table with a neural network for function approximation.\n",
    "   - This would allow handling continuous state spaces more effectively.\n",
    "\n",
    "3. **Advanced Exploration Strategies**:\n",
    "   - Implement techniques like intrinsic motivation or uncertainty-based exploration.\n",
    "   - Use count-based exploration bonuses for less-visited states.\n",
    "\n",
    "4. **Multi-step Learning**:\n",
    "   - Implement n-step Q-learning or TD(λ) to improve learning efficiency.\n",
    "\n",
    "When extending the agent, remember to modify the `save_qlearning_agent()` function accordingly to properly save your enhanced implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training Results\n",
    "\n",
    "Let's visualize how our agent improved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Calculate rolling averages\u001b[39;00m\n\u001b[32m      4\u001b[39m window_size = \u001b[32m10\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m rolling_rewards = np.convolve(\u001b[43mrewards_history\u001b[49m, np.ones(window_size)/window_size, mode=\u001b[33m'\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m rolling_steps = np.convolve(steps_history, np.ones(window_size)/window_size, mode=\u001b[33m'\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m rolling_success = np.convolve([\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m success_history], np.ones(window_size)/window_size, mode=\u001b[33m'\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m'\u001b[39m) * \u001b[32m100\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'rewards_history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate rolling averages\n",
    "window_size = 10\n",
    "rolling_rewards = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_steps = np.convolve(steps_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_success = np.convolve([1 if s else 0 for s in success_history], np.ones(window_size)/window_size, mode='valid') * 100\n",
    "\n",
    "# Create the plots\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# Plot rewards\n",
    "ax1.plot(rolling_rewards)\n",
    "ax1.set_ylabel('Average Reward')\n",
    "ax1.set_title('Training Progress (10-episode rolling average)')\n",
    "\n",
    "# Plot steps\n",
    "ax2.plot(rolling_steps)\n",
    "ax2.set_ylabel('Average Steps')\n",
    "\n",
    "# Plot success rate\n",
    "#ax3.plot(rolling_success)\n",
    "#ax3.set_ylabel('Success Rate (%)')\n",
    "#ax3.set_xlabel('Episode')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Trained Agent\n",
    "\n",
    "Now let's evaluate our trained agent with exploration turned off to see how well it performs on unseen seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Turn off exploration for evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43magent\u001b[49m.exploration_rate = \u001b[32m0\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create test environment\u001b[39;00m\n\u001b[32m      5\u001b[39m test_env = SailingEnv(**get_initial_windfield(\u001b[33m'\u001b[39m\u001b[33mtraining_1\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "# Turn off exploration for evaluation\n",
    "agent.exploration_rate = 0\n",
    "\n",
    "# Create test environment\n",
    "test_env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Test parameters\n",
    "num_test_episodes = 5\n",
    "max_steps = 1000\n",
    "\n",
    "print(\"Testing the trained agent on 5 new episodes...\")\n",
    "# Testing loop\n",
    "for episode in range(num_test_episodes):\n",
    "    # Reset environment\n",
    "    observation, info = test_env.reset(seed=1000 + episode)  # Different seeds from training\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action using learned policy\n",
    "        action = mon_agent.act(observation)\n",
    "        observation, reward, done, truncated, info = test_env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Test Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Your Agent's Behavior\n",
    "\n",
    "While training metrics provide quantitative insights, actually seeing your agent navigate through the environment can reveal critical information about its behavior and strategy.\n",
    "\n",
    "#### Using evaluate_agent.ipynb for Visualization\n",
    "\n",
    "The `evaluate_agent.ipynb` notebook provides powerful visualization tools that let you see:\n",
    "- Complete trajectories across different initial windfields\n",
    "- How your agent responds to wind conditions\n",
    "- Frame-by-frame animations of navigation decisions\n",
    "\n",
    "To use these visualizations, you'll need to save your agent in the proper format first, which we'll do in the next section. Once saved, you can:\n",
    "\n",
    "1. Open `evaluate_agent.ipynb`\n",
    "2. Set `AGENT_PATH` to your saved agent file\n",
    "3. Run the evaluation cells to generate visualizations\n",
    "\n",
    "These visual insights can help you identify patterns, diagnose issues, and refine your agent's strategy in ways that metrics alone cannot reveal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Your Agent for Submission\n",
    "\n",
    "Now let's save our trained agent as a Python file that can be used for evaluation and submission. This step is crucial for three key reasons:\n",
    "\n",
    "1. **Visualization and Testing**: Saving allows you to use the `evaluate_agent.ipynb` notebook to visualize trajectories and test performance across different scenarios.\n",
    "\n",
    "2. **Validation and Evaluation**: The saved agent can be validated with `validate_agent.ipynb` and thoroughly evaluated using different seeds and initial windfields with `evaluate_agent.ipynb`. These notebooks provide important metrics and visualizations to understand your agent's performance.\n",
    "\n",
    "3. **Submission Format**: Any agent submitted to the evaluator **must** be in this format - a single standalone Python (.py) file with a class that inherits from `BaseAgent` and implements all required methods. This is the official submission format for the challenge.\n",
    "\n",
    "For Q-learning agents like ours, we've created a utility function `save_qlearning_agent()` in `src/utils/agent_utils.py` that handles the process of saving the agent with all its learned parameters. This creates a standalone Python file ready for submission.\n",
    "\n",
    "This utility function:\n",
    "1. Extracts the Q-table from your trained agent\n",
    "2. Creates a new Python file with a clean agent implementation\n",
    "3. Embeds the learned Q-values directly in the code\n",
    "4. Includes all the necessary methods (act, reset, seed, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved to ../src/agents/new_agent_4_trained.py\n",
      "The file contains 5739 state-action pairs.\n",
      "You can now use this file with validate_agent.ipynb and evaluate_agent.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import the utility function for saving Q-learning agents\n",
    "from src.utils.agent_utils import save_qlearning_agent\n",
    "\n",
    "# Save our trained agent\n",
    "save_qlearning_agent(\n",
    "    agent=mon_agent,\n",
    "    output_path=f\"../src/agents/new_agent_4_trained.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Utility for Your Own Agents\n",
    "\n",
    "If you implement different types of agents (such as DQN, SARSA, or custom algorithms), you may need to create similar utility functions. Some tips:\n",
    "\n",
    "1. Make sure your save function preserves all necessary parameters and learned values\n",
    "2. Store them in a way that doesn't require additional files (embedded in the code)\n",
    "3. Ensure the saved agent includes all required methods from the BaseAgent interface\n",
    "\n",
    "When extending `save_qlearning_agent()` for different Q-learning variants, you might need to:\n",
    "- Update the state discretization logic\n",
    "- Change how parameters are stored and initialized\n",
    "- Modify the act() method's logic for your specific algorithm\n",
    "\n",
    "### Agent Types and Saving Strategies\n",
    "\n",
    "**For Rule-Based Agents:**\n",
    "- Since rule-based agents don't have learned parameters, you can simply ensure your agent class follows the `BaseAgent` interface\n",
    "- Implement all required methods: `__init__()`, `act(observation)`, `reset()`, and `seed(seed)`\n",
    "- These are typically the simplest to save as the agent's logic is entirely defined in the code\n",
    "\n",
    "**For Deep Learning-Based Agents:**\n",
    "- Include the model architecture definition directly in your Python file\n",
    "- Convert model weights to numpy arrays and include them in your code\n",
    "- Add functions to rebuild the model from these arrays\n",
    "\n",
    "### Key Requirements for Any Submission File\n",
    "\n",
    "Regardless of your agent type, ensure your submission file:\n",
    "1. **Contains everything**: All code, parameters, and data needed to run the agent\n",
    "2. **Is a single file**: No external dependencies beyond standard libraries\n",
    "3. **Follows the interface**: Properly inherits from `BaseAgent` and implements all required methods\n",
    "4. **Requires no arguments**: The agent must initialize without any required arguments\n",
    "5. **Is deterministic**: For a given seed, the agent should behave identically each time\n",
    "\n",
    "## Important Note on Import Paths\n",
    "\n",
    "When creating agent files for submission, make sure to use the correct import paths:\n",
    "- **Use**: `from agents.base_agent import BaseAgent`\n",
    "- **Not**: `from src.agents.base_agent import BaseAgent`\n",
    "\n",
    "This is because the validation and evaluation scripts run from within the `src` directory, so imports should be relative to that location. Our utility function `save_qlearning_agent` already handles this for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Agent Training and Evaluation\n",
    "\n",
    "The approach we've taken here is interactive and educational, but for serious agent development, you'll likely want to automate the training process. Here's where you could expand:\n",
    "\n",
    "```python\n",
    "# Your training script could look something like this:\n",
    "def train_agent(agent, initial_windfields, num_episodes, save_path):\n",
    "    # Setup training parameters\n",
    "    # ...\n",
    "    \n",
    "    # Train on multiple initial windfields\n",
    "    for initial_windfield_name, initial_windfield in initial_windfields.items():\n",
    "        # Train agent on this initial_windfields\n",
    "        # ...\n",
    "        \n",
    "    # Save the trained agent\n",
    "    # ...\n",
    "    \n",
    "    return training_metrics\n",
    "```\n",
    "\n",
    "Creating a command-line interface for training and evaluation would allow you to:\n",
    "1. Train agents with different hyperparameters\n",
    "2. Evaluate on multiple initial_windfields \n",
    "3. Create systematic experiments\n",
    "\n",
    "This is left as an exercise for you to implement based on your specific approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **Agent Requirements**: Understanding the BaseAgent interface \n",
    "2. **The Naive Agent**: Examining a simple rule-based agent \n",
    "3. **Simplified Q-Learning Agent**: Implementing and training a basic RL agent that uses only local information (position, velocity, and local wind)\n",
    "\n",
    "### Next Steps for Developing Your Own Agent\n",
    "\n",
    "Now it's your turn to develop your own agent. Here are some suggestions:\n",
    "\n",
    "1. **Enhance the Q-Learning Agent**:\n",
    "   - Extend the state representation to incorporate the full wind field (not just local wind)\n",
    "   - This would allow the agent to anticipate wind changes and plan better routes\n",
    "   - Hint: Modify the `discretize_state` method to extract and process relevant features from the flattened wind field\n",
    "\n",
    "2. **Algorithmic Improvements**:\n",
    "   - Implement function approximation to handle continuous state spaces better\n",
    "   - Explore other RL algorithms like SARSA, Expected SARSA, or Deep Q-Networks\n",
    "   - Experiment with different exploration strategies that adapt over time\n",
    "\n",
    "3. **Physics-Based Approaches**:\n",
    "   - Leverage your understanding of sailing physics (from challenge_walkthrough notebook)\n",
    "   - Implement rule-based algorithms or path planning (A*, etc.) that take advantage of domain knowledge\n",
    "   - Create hybrid approaches that combine RL with domain-specific rules\n",
    "   \n",
    "### Validating and Evaluating Your Agent\n",
    "\n",
    "After you've developed your agent, the next steps are to:\n",
    "\n",
    "1. **Validate your agent** using the `validate_agent.ipynb` notebook or command-line tool\n",
    "2. **Evaluate your agent** using the `evaluate_agent.ipynb` notebook\n",
    "\n",
    "Remember that agents combining multiple techniques often perform best - consider how you might blend RL with domain knowledge of sailing physics for optimal results!\n",
    "\n",
    "Good luck with the Sailing Challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
