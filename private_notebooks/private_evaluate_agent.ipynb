{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Private Submission Evaluation (Evaluator Only)\n",
    "\n",
    "This notebook provides a comprehensive evaluation system for the Sailing Challenge submissions. It is **for evaluator use only** and differs from the student version in that it:\n",
    "\n",
    "1. Includes evaluation on the hidden test scenario\n",
    "2. Provides more detailed diagnostic information\n",
    "3. Compares performance across all scenarios including the private test scenario\n",
    "\n",
    "Use this notebook to evaluate student submissions and determine their ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up the evaluation environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available evaluation scenarios:\n",
      "- simple_test\n",
      "- test (HIDDEN TEST)\n",
      "- training_1\n",
      "- training_2\n",
      "- training_3\n"
     ]
    }
   ],
   "source": [
    "# Initial imports and setup\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the environment and evaluation modules\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))  # Add project root to path\n",
    "\n",
    "from env_sailing import SailingEnv\n",
    "from evaluation import evaluate_agent, visualize_trajectory\n",
    "from agents.base_agent import BaseAgent\n",
    "from scenarios import get_scenario\n",
    "from scenarios.private_scenarios import TEST_SCENARIO, ALL_SCENARIOS\n",
    "from src.test_agent_validity import validate_agent\n",
    "\n",
    "# List available scenarios including the test scenario\n",
    "print(\"Available evaluation scenarios:\")\n",
    "for scenario_name in sorted(ALL_SCENARIOS.keys()):\n",
    "    print(f\"- {scenario_name}\" + (\" (HIDDEN TEST)\" if scenario_name == \"test\" else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Set your evaluation parameters below. You can easily modify these values without changing the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission to evaluate: ../src/agents/agent_greedy.py\n",
      "Using 10 seeds: 1...10\n",
      "Max steps per episode: 200\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "### MODIFY THESE PARAMETERS AS NEEDED ######\n",
    "#############################################\n",
    "\n",
    "# Path to the submission file (change this to evaluate different submissions)\n",
    "SUBMISSION_PATH = \"../src/agents/agent_greedy.py\"\n",
    "\n",
    "# Number of seeds to use for evaluation\n",
    "# For quick testing, use a small number (e.g., 10)\n",
    "# For final evaluation, use a larger number (e.g., 100)\n",
    "EVAL_SEEDS = list(range(1, 11))  # Seeds 1-10 for quick testing\n",
    "# EVAL_SEEDS = list(range(1, 101))  # Uncomment for full evaluation (seeds 1-100)\n",
    "\n",
    "# Maximum steps per episode\n",
    "MAX_HORIZON = 200\n",
    "\n",
    "# Whether to show visualization of agent trajectories\n",
    "ENABLE_VISUALIZATION = True\n",
    "\n",
    "# Whether to show progress bars during evaluation\n",
    "VERBOSE = True\n",
    "\n",
    "#############################################\n",
    "### DO NOT MODIFY BELOW THIS LINE ##########\n",
    "#############################################\n",
    "\n",
    "print(f\"Submission to evaluate: {SUBMISSION_PATH}\")\n",
    "print(f\"Using {len(EVAL_SEEDS)} seeds: {EVAL_SEEDS[0]}...{EVAL_SEEDS[-1]}\")\n",
    "print(f\"Max steps per episode: {MAX_HORIZON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Validate Submission\n",
    "\n",
    "First, let's load and validate the student's submission to ensure it meets the required interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Validation successful: Agent 'GreedyAgent' loaded from ../src/agents/agent_greedy.py\n",
      "Agent loaded successfully. Ready for evaluation.\n"
     ]
    }
   ],
   "source": [
    "def load_submission(submission_path):\n",
    "    \"\"\"Load a student submission from a Python file.\n",
    "    \n",
    "    Args:\n",
    "        submission_path: Path to the submission file\n",
    "        \n",
    "    Returns:\n",
    "        An instance of the agent class defined in the submission file\n",
    "    \"\"\"\n",
    "    # First, validate the agent implementation\n",
    "    validation_results = validate_agent(submission_path)\n",
    "    \n",
    "    if not validation_results['valid']:\n",
    "        print(f\"⚠️ VALIDATION FAILED: The agent does not meet requirements!\")\n",
    "        for i, error in enumerate(validation_results['errors'], 1):\n",
    "            print(f\"  Error {i}: {error}\")\n",
    "        \n",
    "        if validation_results['warnings']:\n",
    "            print(\"\\nWarnings:\")\n",
    "            for i, warning in enumerate(validation_results['warnings'], 1):\n",
    "                print(f\"  Warning {i}: {warning}\")\n",
    "                \n",
    "        print(\"\\nPlease fix these issues before evaluation.\")\n",
    "        return None\n",
    "    \n",
    "    # If validation passed, create an instance of the agent\n",
    "    agent_class = validation_results['agent_class']\n",
    "    agent = agent_class()\n",
    "    \n",
    "    print(f\"✓ Validation successful: Agent '{agent_class.__name__}' loaded from {submission_path}\")\n",
    "    \n",
    "    # Print any warnings\n",
    "    if validation_results['warnings']:\n",
    "        print(\"\\nWarnings (non-critical):\")\n",
    "        for i, warning in enumerate(validation_results['warnings'], 1):\n",
    "            print(f\"  Warning {i}: {warning}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Load the agent\n",
    "agent = load_submission(SUBMISSION_PATH)\n",
    "\n",
    "if agent is None:\n",
    "    print(\"Evaluation aborted due to validation failure.\")\n",
    "else:\n",
    "    print(\"Agent loaded successfully. Ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Evaluation Scenarios\n",
    "\n",
    "Now let's prepare the scenarios we'll use to evaluate the agent, including both training scenarios and the hidden test scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared evaluation scenarios:\n",
      "  - TRAINING_1\n",
      "  - TRAINING_2\n",
      "  - TRAINING_3\n",
      "  - TEST (HIDDEN TEST)\n"
     ]
    }
   ],
   "source": [
    "# Set visualization parameters for scenarios\n",
    "viz_params = {\n",
    "    'env_params': {\n",
    "        'wind_grid_density': 25,    # Fewer arrows = clearer visualization\n",
    "        'wind_arrow_scale': 80,     # Larger value = smaller arrows\n",
    "        'render_mode': \"rgb_array\" if ENABLE_VISUALIZATION else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define all evaluation scenarios\n",
    "evaluation_scenarios = {\n",
    "    \"training_1\": get_scenario(\"training_1\"),\n",
    "    \"training_2\": get_scenario(\"training_2\"),\n",
    "    \"training_3\": get_scenario(\"training_3\"),\n",
    "    \"test\": TEST_SCENARIO  # The hidden test scenario\n",
    "}\n",
    "\n",
    "# Add visualization parameters to each scenario\n",
    "for scenario_name, scenario in evaluation_scenarios.items():\n",
    "    scenario.update(viz_params)\n",
    "\n",
    "print(\"Prepared evaluation scenarios:\")\n",
    "for name in evaluation_scenarios.keys():\n",
    "    print(f\"  - {name.upper()}\" + (\" (HIDDEN TEST)\" if name == \"test\" else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Agent on All Scenarios\n",
    "\n",
    "Let's evaluate the agent on all scenarios, including the hidden test scenario. We'll run multiple episodes with different seeds for robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on TRAINING_1 scenario...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be00937f5eb4798a96c7b0f33ceaed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating seeds:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean Reward: 10.96 ± 11.92\n",
      "  Success Rate: 50.00%\n",
      "  Mean Steps: 177.9 ± 28.9\n",
      "  ⚠️ Failed on 5 out of 10 episodes\n",
      "\n",
      "Evaluating on TRAINING_2 scenario...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e368b5f1284c43931c3abdc10bc00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating seeds:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean Reward: 14.10 ± 9.87\n",
      "  Success Rate: 70.00%\n",
      "  Mean Steps: 173.7 ± 23.7\n",
      "  ⚠️ Failed on 3 out of 10 episodes\n",
      "\n",
      "Evaluating on TRAINING_3 scenario...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd7dc830daf47d3941bb02fdaec5a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating seeds:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean Reward: 0.00 ± 0.00\n",
      "  Success Rate: 0.00%\n",
      "  Mean Steps: 200.0 ± 0.0\n",
      "  ⚠️ Failed on 10 out of 10 episodes\n",
      "\n",
      "Evaluating on TEST scenario...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ded04d2913745ed868acafae9ada092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating seeds:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean Reward: 0.00 ± 0.00\n",
      "  Success Rate: 0.00%\n",
      "  Mean Steps: 200.0 ± 0.0\n",
      "  ⚠️ Failed on 10 out of 10 episodes\n"
     ]
    }
   ],
   "source": [
    "# Only run if the agent was successfully loaded\n",
    "if 'agent' in locals():\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    demo_results = {}\n",
    "    \n",
    "    # Evaluate on each scenario\n",
    "    for scenario_name, scenario in evaluation_scenarios.items():\n",
    "        print(f\"\\nEvaluating on {scenario_name.upper()} scenario...\")\n",
    "        \n",
    "        # Run evaluation with multiple seeds\n",
    "        results = evaluate_agent(\n",
    "            agent=agent,\n",
    "            scenario=scenario,\n",
    "            seeds=EVAL_SEEDS,\n",
    "            max_horizon=MAX_HORIZON,\n",
    "            verbose=VERBOSE,\n",
    "            render=False  # Don't render during bulk evaluation\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[scenario_name] = results\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(f\"  Mean Reward: {results['mean_reward']:.2f} ± {results['std_reward']:.2f}\")\n",
    "        print(f\"  Success Rate: {results['success_rate']:.2%}\")\n",
    "        print(f\"  Mean Steps: {results['mean_steps']:.1f} ± {results['std_steps']:.1f}\")\n",
    "        \n",
    "        # Store detailed individual episode results\n",
    "        if 'individual_results' in results:\n",
    "            successes = [ep['success'] for ep in results['individual_results']]\n",
    "            failures = len(successes) - sum(successes)\n",
    "            if failures > 0:\n",
    "                print(f\"  ⚠️ Failed on {failures} out of {len(successes)} episodes\")\n",
    "                \n",
    "        # Run one demo episode with rendering for visualization if enabled\n",
    "        if ENABLE_VISUALIZATION:\n",
    "            demo_seed = EVAL_SEEDS[0]  # Use the first seed for demonstration\n",
    "            demo_result = evaluate_agent(\n",
    "                agent=agent,\n",
    "                scenario=scenario,\n",
    "                seeds=[demo_seed],\n",
    "                max_horizon=MAX_HORIZON,\n",
    "                verbose=False,\n",
    "                render=True,\n",
    "                full_trajectory=True\n",
    "            )\n",
    "                \n",
    "            demo_results[scenario_name] = demo_result\n",
    "else:\n",
    "    print(\"Evaluation skipped due to agent loading failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Summary and Analysis\n",
    "\n",
    "This section provides a detailed summary of the agent's performance across all scenarios, with special attention to performance on the hidden test scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Success Rate</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Mean Steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAINING_1</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>10.96 ± 11.92</td>\n",
       "      <td>177.9 ± 28.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAINING_2</td>\n",
       "      <td>70.00%</td>\n",
       "      <td>14.10 ± 9.87</td>\n",
       "      <td>173.7 ± 23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAINING_3</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>200.0 ± 0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST (TEST)</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>200.0 ± 0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scenario Success Rate    Mean Reward    Mean Steps\n",
       "0   TRAINING_1       50.00%  10.96 ± 11.92  177.9 ± 28.9\n",
       "1   TRAINING_2       70.00%   14.10 ± 9.87  173.7 ± 23.7\n",
       "2   TRAINING_3        0.00%    0.00 ± 0.00   200.0 ± 0.0\n",
       "3  TEST (TEST)        0.00%    0.00 ± 0.00   200.0 ± 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Analysis:\n",
      "  Average Success Rate: 30.00%\n",
      "  Average Reward: 6.26\n",
      "  Test Scenario Success Rate: 0.00%\n",
      "  Test Scenario Reward: 0.00\n",
      "\n",
      "Weighted Score (50% test, 50% training):\n",
      "  Weighted Success Rate: 20.00%\n",
      "  Weighted Reward: 4.18\n",
      "\n",
      "Overall Grade: F\n"
     ]
    }
   ],
   "source": [
    "# Only run if we have evaluation results\n",
    "if 'all_results' in locals() and all_results:\n",
    "    # Create summary table for all scenarios\n",
    "    summary_data = []\n",
    "    for scenario_name, results in all_results.items():\n",
    "        summary_data.append({\n",
    "            'Scenario': scenario_name.upper() + (\" (TEST)\" if scenario_name == \"test\" else \"\"),\n",
    "            'Success Rate': f\"{results['success_rate']:.2%}\",\n",
    "            'Mean Reward': f\"{results['mean_reward']:.2f} ± {results['std_reward']:.2f}\",\n",
    "            'Mean Steps': f\"{results['mean_steps']:.1f} ± {results['std_steps']:.1f}\"\n",
    "        })\n",
    "    \n",
    "    # Create and display summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Calculate average performance metrics\n",
    "    avg_success_rate = np.mean([results['success_rate'] for results in all_results.values()])\n",
    "    avg_reward = np.mean([results['mean_reward'] for results in all_results.values()])\n",
    "    avg_steps = np.mean([results['mean_steps'] for results in all_results.values()])\n",
    "    \n",
    "    # Calculate test scenario performance specifically\n",
    "    test_success_rate = all_results['test']['success_rate'] if 'test' in all_results else 0\n",
    "    test_reward = all_results['test']['mean_reward'] if 'test' in all_results else 0\n",
    "    \n",
    "    # Calculate overall weighted score (50% test, 50% training average)\n",
    "    if 'test' in all_results:\n",
    "        training_success = np.mean([\n",
    "            results['success_rate'] for name, results in all_results.items() \n",
    "            if name != 'test'\n",
    "        ])\n",
    "        training_reward = np.mean([\n",
    "            results['mean_reward'] for name, results in all_results.items() \n",
    "            if name != 'test'\n",
    "        ])\n",
    "        \n",
    "        weighted_success = 0.5 * test_success_rate + 0.5 * training_success\n",
    "        weighted_reward = 0.5 * test_reward + 0.5 * training_reward\n",
    "        \n",
    "        print(\"\\nPerformance Analysis:\")\n",
    "        print(f\"  Average Success Rate: {avg_success_rate:.2%}\")\n",
    "        print(f\"  Average Reward: {avg_reward:.2f}\")\n",
    "        print(f\"  Test Scenario Success Rate: {test_success_rate:.2%}\")\n",
    "        print(f\"  Test Scenario Reward: {test_reward:.2f}\")\n",
    "        print(f\"\\nWeighted Score (50% test, 50% training):\")\n",
    "        print(f\"  Weighted Success Rate: {weighted_success:.2%}\")\n",
    "        print(f\"  Weighted Reward: {weighted_reward:.2f}\")\n",
    "        \n",
    "        # Provide a simple grading metric\n",
    "        grade = \"A+\" if weighted_success > 0.98 else \\\n",
    "                \"A\" if weighted_success > 0.95 else \\\n",
    "                \"B\" if weighted_success > 0.9 else \\\n",
    "                \"C\" if weighted_success > 0.8 else \\\n",
    "                \"D\" if weighted_success > 0.7 else \"F\"\n",
    "                \n",
    "        print(f\"\\nOverall Grade: {grade}\")\n",
    "else:\n",
    "    print(\"No evaluation results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Agent Behavior\n",
    "\n",
    "In this section, we'll visualize how our agent performs in the sailing environment. The visualization will show the agent's trajectory and the wind conditions throughout the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing agent behavior on scenario: training_1\n",
      "Using seed: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3048ec0f01844e498586d9ce5a3fc8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Step:', max=199), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "### MODIFY THESE PARAMETERS AS NEEDED ######\n",
    "#############################################\n",
    "\n",
    "# Set to True to enable visualization\n",
    "VISUALIZE = True\n",
    "\n",
    "# Visualization parameters\n",
    "VIZ_SCENARIO_NAME = \"training_1\"  # Choose which scenario to visualize\n",
    "VIZ_SEED = 42                    # Choose a single seed for visualization\n",
    "\n",
    "#############################################\n",
    "### DO NOT MODIFY BELOW THIS LINE ##########\n",
    "#############################################\n",
    "\n",
    "# Only run if visualization is enabled and agent is loaded\n",
    "if VISUALIZE and 'agent' in locals():\n",
    "    # Get the scenario with visualization parameters\n",
    "    viz_scenario = get_scenario(VIZ_SCENARIO_NAME)\n",
    "    viz_scenario.update({\n",
    "        'env_params': {\n",
    "            'wind_grid_density': 25,\n",
    "            'wind_arrow_scale': 80,\n",
    "            'render_mode': \"rgb_array\"\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    print(f\"Visualizing agent behavior on scenario: {VIZ_SCENARIO_NAME}\")\n",
    "    print(f\"Using seed: {VIZ_SEED}\")\n",
    "    \n",
    "    # Run the evaluation with visualization enabled\n",
    "    viz_results = evaluate_agent(\n",
    "        agent=agent,\n",
    "        scenario=viz_scenario,\n",
    "        seeds=VIZ_SEED,\n",
    "        max_horizon=MAX_HORIZON,\n",
    "        verbose=False,\n",
    "        render=True,\n",
    "        full_trajectory=True  # Enable full trajectory for visualization\n",
    "    )\n",
    "    \n",
    "    # Visualize the trajectory with a slider\n",
    "    visualize_trajectory(viz_results, None, with_slider=True)\n",
    "else:\n",
    "    if 'agent' in locals():\n",
    "        print(\"Visualization is disabled. Set VISUALIZE = True to see agent behavior.\")\n",
    "    else:\n",
    "        print(\"No agent defined. Please run the cells above to create an agent first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Performance Across Scenarios\n",
    "\n",
    "Now let's visualize how our agent performs across different scenarios. This will help us understand the agent's robustness to different wind conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on training_1...\n",
      "Scenario: training_1\n",
      "❌ Failed to reach goal (max 200 steps)\n",
      "Mean Discounted Reward: 0.00\n",
      "\n",
      "Trajectory for training_1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae82ee4b3f842ec89b0338a7cdd834a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Step:', max=199), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on training_2...\n",
      "Scenario: training_2\n",
      "❌ Failed to reach goal (max 200 steps)\n",
      "Mean Discounted Reward: 0.00\n",
      "\n",
      "Trajectory for training_2:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1b3f5f15414580a3a2db7bc7bf6501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Step:', max=199), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on training_3...\n",
      "Scenario: training_3\n",
      "❌ Failed to reach goal (max 200 steps)\n",
      "Mean Discounted Reward: 0.00\n",
      "\n",
      "Trajectory for training_3:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b990835c964be2b922bf3f10b58268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Step:', max=199), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown scenario 'test'. Available scenarios: ['training_1', 'training_2', 'training_3', 'simple_test']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscenario_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Get the scenario with visualization parameters\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m scenario \u001b[38;5;241m=\u001b[39m \u001b[43mget_scenario\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscenario_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m scenario\u001b[38;5;241m.\u001b[39mupdate(viz_params)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n",
      "File \u001b[0;32m~/code_project_v0/Sailing_project_v1/src/scenarios/__init__.py:109\u001b[0m, in \u001b[0;36mget_scenario\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03mGet the parameters for a specific scenario.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    Dictionary containing wind_init_params and wind_evol_params\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SCENARIOS:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown scenario \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Available scenarios: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SCENARIOS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SCENARIOS[name]\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown scenario 'test'. Available scenarios: ['training_1', 'training_2', 'training_3', 'simple_test']"
     ]
    }
   ],
   "source": [
    "# Set to True to enable multi-scenario comparison\n",
    "COMPARE_SCENARIOS = True\n",
    "\n",
    "# Scenarios to compare\n",
    "SCENARIO_NAMES = [\"training_1\", \"training_2\", \"training_3\", \"test\"]\n",
    "COMPARISON_SEED = 42\n",
    "\n",
    "if COMPARE_SCENARIOS and 'agent' in locals():\n",
    "    # Visualization parameters\n",
    "    viz_params = {\n",
    "        'env_params': {\n",
    "            'wind_grid_density': 25,\n",
    "            'wind_arrow_scale': 80,\n",
    "            'render_mode': \"rgb_array\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Store results for each scenario\n",
    "    scenario_results = {}\n",
    "    \n",
    "    # Evaluate agent on each scenario\n",
    "    for scenario_name in SCENARIO_NAMES:\n",
    "        print(f\"\\nEvaluating on {scenario_name}...\")\n",
    "        \n",
    "        # Get the scenario with visualization parameters\n",
    "        scenario = get_scenario(scenario_name)\n",
    "        scenario.update(viz_params)\n",
    "        \n",
    "        # Run the evaluation\n",
    "        results = evaluate_agent(\n",
    "            agent=agent,\n",
    "            scenario=scenario,\n",
    "            seeds=COMPARISON_SEED,\n",
    "            max_horizon=MAX_HORIZON,\n",
    "            verbose=False,\n",
    "            render=True,\n",
    "            full_trajectory=True\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        scenario_results[scenario_name] = results\n",
    "        \n",
    "        # Display performance metrics\n",
    "        rewards = results['rewards']\n",
    "        steps = results['mean_steps']\n",
    "        discounted_reward = results['mean_reward']\n",
    "        success = any(r > 0 for r in rewards)\n",
    "        \n",
    "        print(f\"Scenario: {scenario_name}\")\n",
    "        if success:\n",
    "            print(f\"✅ Success! Reached goal in {steps:.0f} steps\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to reach goal (max {steps:.0f} steps)\")\n",
    "        print(f\"Mean Discounted Reward: {discounted_reward:.2f}\")\n",
    "        \n",
    "        # Visualize the trajectory\n",
    "        print(f\"\\nTrajectory for {scenario_name}:\")\n",
    "        visualize_trajectory(results, None, with_slider=True)\n",
    "        \n",
    "    # Print summary comparison\n",
    "    print(\"\\n===== Scenario Comparison Summary =====\")\n",
    "    for scenario_name, results in scenario_results.items():\n",
    "        success = any(r > 0 for r in results['rewards'])\n",
    "        print(f\"{scenario_name}: {'✅ Success' if success else '❌ Failed'} | \"\n",
    "              f\"Steps: {results['mean_steps']:.0f} | \"\n",
    "              f\"Reward: {results['mean_reward']:.2f}\")\n",
    "else:\n",
    "    if 'agent' in locals():\n",
    "        print(\"Scenario comparison is disabled. Set COMPARE_SCENARIOS = True to compare performance across scenarios.\")\n",
    "    else:\n",
    "        print(\"No agent defined. Please run the cells above to create an agent first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Command-Line Evaluation (For Batch Processing)\n",
    "\n",
    "For comprehensive evaluation of submissions, use the command-line interface with the test scenario:\n",
    "\n",
    "```bash\n",
    "cd Sailing_project_v1/src\n",
    "python3 evaluate_submission.py agents/submission_example.py --include-test --seeds {1..100}\n",
    "```\n",
    "\n",
    "### Command Options\n",
    "\n",
    "- `agents/submission_example.py`: Path to the agent implementation file\n",
    "- `--include-test`: Include the hidden test scenario in evaluation (evaluator use only)\n",
    "- `--seeds {1..100}`: Evaluate on seeds 1 through 100 (bash expansion syntax)\n",
    "- `--max_horizon N`: Maximum steps per episode (default: 200)\n",
    "- `--output FILE`: Save results to a JSON file (e.g., `--output results.json`)\n",
    "- `--verbose`: Show detailed evaluation results (default: simplified output)\n",
    "\n",
    "### Sample Output (Simplified)\n",
    "\n",
    "```\n",
    "Validating agent: agents/submission_example.py\n",
    "✅ Successfully loaded agent: SubmissionAgent\n",
    "Evaluating on 4 scenarios with 100 seeds\n",
    "Max horizon: 200 steps\n",
    "SCENARIO | SUCCESS RATE | MEAN REWARD | MEAN STEPS\n",
    "training_1 | Success: 98.00% | Reward: 61.20 ± 3.22 | Steps: 50.0 ± 5.3\n",
    "training_2 | Success: 97.00% | Reward: 63.84 ± 3.36 | Steps: 45.8 ± 5.3\n",
    "training_3 | Success: 96.00% | Reward: 62.24 ± 3.06 | Steps: 48.3 ± 4.8\n",
    "test (TEST) | Success: 95.00% | Reward: 61.93 ± 2.99 | Steps: 48.8 ± 4.9\n",
    "======================================================================\n",
    "OVERALL | 96.50% ± 1.12% | 62.30 ± 0.96 | 48.2 ± 1.5\n",
    "WEIGHTED FINAL REWARD: 62.18 (50% test, 50% training)\n",
    "======================================================================\n",
    "```\n",
    "\n",
    "\n",
    "### Evaluate Only on Test Scenario\n",
    "\n",
    "To evaluate an agent exclusively on the hidden test scenario:\n",
    "\n",
    "```bash\n",
    "cd Sailing_project_v1/src\n",
    "python3 evaluate_submission.py agents/agent_greedy.py --scenario test --seeds {1..100}\n",
    "```\n",
    "\n",
    "### Evaluate on All Scenarios (Without Training Output)\n",
    "\n",
    "To evaluate on all scenarios and save results to a file without displaying detailed output:\n",
    "\n",
    "```bash\n",
    "cd Sailing_project_v1/src\n",
    "python3 evaluate_submission.py agents/agent_greedy.py --include-test --seeds {1..100} --output results.json\n",
    "```\n",
    "\n",
    "The **weighted final reward** (combining 50% test and 50% training performance) is the key metric for ranking submissions. This metric ensures that agents are evaluated both on their ability to handle the training scenarios and to generalize to new conditions in the test scenario.\n",
    "\n",
    "For detailed seed-by-seed results, especially useful when analyzing failures, add the `--verbose` flag to the command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This private evaluation framework provides a comprehensive assessment of sailing agents with these key metrics:\n",
    "\n",
    "- **Mean reward** is the primary performance indicator, reflecting sailing efficiency\n",
    "- **Standard deviation** across seeds indicates agent robustness and consistency\n",
    "- **Weighted final reward** (50% test + 50% training) provides the definitive ranking metric\n",
    "\n",
    "When evaluating submissions:\n",
    "1. Run batch evaluations with 100+ seeds for statistical significance\n",
    "2. Examine performance gaps between training and test scenarios to assess generalization\n",
    "3. Use `--verbose` and visualization for qualitative assessment of sailing strategies\n",
    "4. Consider reward distribution across different seeds to identify edge cases\n",
    "\n",
    "The private test scenario ensures agents are evaluated on their ability to handle novel conditions, not just memorize training scenarios. This comprehensive evaluation approach rewards agents that implement robust, adaptable sailing strategies rather than brittle, overfitted solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
